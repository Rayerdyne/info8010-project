{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO8010: Project\n",
    "Detailed description: see [README.md](README.md)\n",
    "\n",
    "NB: references in this notebook point towards the README's references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble\n",
    "\n",
    "Importing torch and required libraries, plus miscellaneous definitions\n",
    "\n",
    "### 0.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:32.015091Z",
     "iopub.status.busy": "2022-05-10T09:47:32.014792Z",
     "iopub.status.idle": "2022-05-10T09:47:34.295371Z",
     "shell.execute_reply": "2022-05-10T09:47:34.294579Z",
     "shell.execute_reply.started": "2022-05-10T09:47:32.015017Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torchdata\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as imgtransforms\n",
    "\n",
    "from typing import Tuple\n",
    "from math import ceil\n",
    "\n",
    "from torchvision import models as imgmodels\n",
    "\n",
    "from data import SpecDataset, N_MELS, HOP_LENGTHS\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Audio player & device\n",
    "\n",
    "As this features does not work on VS Code, here is a work-around stolen from [here](https://github.com/microsoft/vscode-jupyter/issues/1012#issuecomment-785410064) to have a working audio player on VSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:34.297407Z",
     "iopub.status.busy": "2022-05-10T09:47:34.296943Z",
     "iopub.status.idle": "2022-05-10T09:47:34.331814Z",
     "shell.execute_reply": "2022-05-10T09:47:34.330820Z",
     "shell.execute_reply.started": "2022-05-10T09:47:34.297382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751fba42bd684eeb879896337698615d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='VS Code user ?', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vsc_check = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"VS Code user ?\",\n",
    "    disabled=False,\n",
    "    tooltip=\"Do you use VS Code to see this notebook ?\",\n",
    "    indent=False,\n",
    ")\n",
    "\n",
    "display(vsc_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:34.333691Z",
     "iopub.status.busy": "2022-05-10T09:47:34.333036Z",
     "iopub.status.idle": "2022-05-10T09:47:34.339749Z",
     "shell.execute_reply": "2022-05-10T09:47:34.339068Z",
     "shell.execute_reply.started": "2022-05-10T09:47:34.333662Z"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import json\n",
    "\n",
    "def VSC_Audio(audio: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    Use instead of IPython.display.Audio as a workaround for VS Code.\n",
    "    `audio` is an array with shape (channels, samples) or just (samples,) for mono.\n",
    "    \"\"\"\n",
    "\n",
    "    if np.ndim(audio) == 1:\n",
    "        channels = [audio.tolist()]\n",
    "    else:\n",
    "        channels = audio.tolist()\n",
    "\n",
    "    return ipd.HTML(\"\"\"\n",
    "        <script>\n",
    "            if (!window.audioContext) {\n",
    "                window.audioContext = new AudioContext();\n",
    "                window.playAudio = function(audioChannels, sr) {\n",
    "                    const buffer = audioContext.createBuffer(audioChannels.length, audioChannels[0].length, sr);\n",
    "                    for (let [channel, data] of audioChannels.entries()) {\n",
    "                        buffer.copyToChannel(Float32Array.from(data), channel);\n",
    "                    }\n",
    "            \n",
    "                    const source = audioContext.createBufferSource();\n",
    "                    source.buffer = buffer;\n",
    "                    source.connect(audioContext.destination);\n",
    "                    source.start();\n",
    "                }\n",
    "            }\n",
    "        </script>\n",
    "        <button onclick=\"playAudio(%s, %s)\">Play</button>\n",
    "    \"\"\" % (json.dumps(channels), sr))\n",
    "\n",
    "def NonVSC_Audio(audio, sr):\n",
    "    return ipd.Audio(audio, rate=sr)\n",
    "\n",
    "audio_player = VSC_Audio if vsc_check.value else NonVSC_Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "First, define:\n",
    "1. the transformer used to pre-process the data\n",
    "1. the loader to be used to load the data for the GTZAN dataset (to start with, to be patched if not sufficient)\n",
    "\n",
    "### 1.1. Transformer\n",
    "\n",
    "**Reminder**: Mel spectrogram \"algo\"\n",
    "1. Take a window of `win_length` samples of the input at current position\n",
    "2. Compute fft with frequency spectrum is sepatated in `n_mels` evenly spaced frequencies, then apply mel scale on it (log-like), use `n_fft` samples for that computation\n",
    "3. Append the computed column to the result\n",
    "4. Move the window of `hop_length`\n",
    "5. Restart in 1.\n",
    "\n",
    "For an input of length `N`, this results in a matrix that has:\n",
    "- `N // hop_length + 1` columns\n",
    "- `n_mels` rows\n",
    "\n",
    "In our case, as input sequence are cropped at `sequence_duration` seconds, aim at:\n",
    "- Width: `ceil(sequence_duration × sampling_rate / hop_length)` columns\n",
    "- Height: `n_mels`\n",
    "\n",
    "#### Variable input length support\n",
    "\n",
    "Later on, we will require the spectrogram length (number of columns) to express like `L = 16×n + 1` so that resnet behaves well. We will directly crop the input waveform at the right length, so that we avoid to zero-pad the spectrograms what would mean an abrupt, unwanted end of the music.\n",
    "\n",
    "We get: `L = N // hop_length + 1 = 16×n+1`\n",
    "\n",
    "Hence:  `N // hop_length = 16×n`\n",
    "\n",
    "Thus:   `N = (16×hop_length)×n+r`, where `r` is an integer in the interval `[0, hop_length[`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:34.342094Z",
     "iopub.status.busy": "2022-05-10T09:47:34.341291Z",
     "iopub.status.idle": "2022-05-10T09:47:34.352882Z",
     "shell.execute_reply": "2022-05-10T09:47:34.351544Z",
     "shell.execute_reply.started": "2022-05-10T09:47:34.342067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duration of the sequence to randomly crop out of the original sequence\n",
    "sequence_durations = (20, 27)    # s\n",
    "\n",
    "# parameters value taken from reference [1]\n",
    "gtzan_sampling_rate = 22050 # Hz\n",
    "sampling_rate = 12000      # 12 kHz\n",
    "\n",
    "# sampling_rate is a multiple of 16 so it's fine if seconds are even for \n",
    "# the spectrograms to shape like 16×n+1 for the resnet nice behaviour\n",
    "min_spec_length_seconds = 20\n",
    "max_spec_length_seconds = 28\n",
    "min_spec_length, max_spec_length = [int((x * sampling_rate) / HOP_LENGTHS[1] + 1) for x in [\n",
    "    min_spec_length_seconds, max_spec_length_seconds\n",
    "]]\n",
    "\n",
    "class OurRandomCrop(object):\n",
    "    \"\"\"\n",
    "    Cannot work because of concurrency, and have to stack tensors of same dimensions\n",
    "    in a same minibatch...\n",
    "    \"\"\"\n",
    "    def __init__(self, height: int, bounds: Tuple[int, int], step: int = 1):\n",
    "        self.height = height\n",
    "        self.base = bounds[0]\n",
    "        self.num = int(ceil((bounds[1] - bounds[0]) / step))\n",
    "        self.step = step\n",
    "        self.batch_size = 5\n",
    "        self.counter = 0\n",
    "    \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.counter == 0:\n",
    "            self.counter = self.batch_size - 1\n",
    "            self.r = int(torch.randint(self.num, (1, 1)))\n",
    "        self.counter = (self.counter + 1) % 5\n",
    "        l = self.base + self.r * self.step\n",
    "        return imgtransforms.RandomCrop((self.height, l))(x)\n",
    "    \n",
    "\n",
    "def make_spec_transform(spec_length: int):\n",
    "    return imgtransforms.Compose([\n",
    "        imgtransforms.RandomCrop((N_MELS, spec_length)),\n",
    "        imgtransforms.RandomApply([\n",
    "            imgtransforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.4, hue=0.1),\n",
    "            imgtransforms.GaussianBlur(3, sigma=(0.1, 1)),\n",
    "            imgtransforms.RandomErasing(p=0.4)\n",
    "        ], p=0.7)\n",
    "    ])\n",
    "\n",
    "transform = make_spec_transform(min_spec_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset and data loader\n",
    "\n",
    "We split the original dataset (1000 sequences long) into test and train set:\n",
    "- Training set will be 70% of the dataset\n",
    "- Test set will contain the remaining sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:34.355378Z",
     "iopub.status.busy": "2022-05-10T09:47:34.355029Z",
     "iopub.status.idle": "2022-05-10T09:47:34.383178Z",
     "shell.execute_reply": "2022-05-10T09:47:34.382387Z",
     "shell.execute_reply.started": "2022-05-10T09:47:34.355338Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_workers = 1\n",
    "\n",
    "dataset = SpecDataset(\n",
    "    \"data/gtzan\",\n",
    "    sampling_rate=sampling_rate,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "trainset_length = int(0.7 * dataset_length)\n",
    "testset_length = dataset_length - trainset_length\n",
    "trainset, testset = torchdata.random_split(dataset, [trainset_length, testset_length])\n",
    "\n",
    "data_loader = torchdata.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "train_loader = torchdata.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torchdata.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: an element `spec` of the dataset has the dimensions: (batch, channel, frequency, time), where\n",
    "- batch correspond to the id of the sample in the batch\n",
    "- channel is artificial construction to use resnet models, spans across the different spectrograms\n",
    "- frequency spans across the frequency range of the window\n",
    "- time indicate the time of the window\n",
    "\n",
    "### 1.3. Test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:34.384440Z",
     "iopub.status.busy": "2022-05-10T09:47:34.384242Z",
     "iopub.status.idle": "2022-05-10T09:47:36.998699Z",
     "shell.execute_reply": "2022-05-10T09:47:36.997715Z",
     "shell.execute_reply.started": "2022-05-10T09:47:34.384415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec shape: torch.Size([5, 3, 128, 801])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff608258d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAABYCAYAAAD83SBJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABRqElEQVR4nO29d7gdV3nv/1nTdj+9qzfLvWGDTbAxphsChBACSS7lQpwYUkhISL0k/H65N5BALiEJxZQEQm8BQrExpppiW8ZFLrK6jqTT6+57T1n3j3dm9pyjI1m2ZesY7+/z6NE+s2fPvLNmrbe/71Jaa9poo4022njywTjdBLTRRhtttHF60BYAbbTRRhtPUrQFQBtttNHGkxRtAdBGG2208SRFWwC00UYbbTxJ0RYAbbTRRhtPUjwmAkAp9QKl1INKqb1KqT9/LO7RRhtttNHGo4M61XUASikT2A08FzgC3A68Wmt9/ym9URtttNFGG48Kj4UF8FRgr9Z6v9a6CXwWeOljcJ822mijjTYeBazH4JprgMOJv48ATzvRD8xcTttdPSgf7KoGrQksBSi0CcoHFYSWigZtKVSg0YYCln6X/N/t15ybn2VnqZeM45I3G8zOdhCkNQSK1HyANhXmUJOmb+J7JmZZoS0wmxCYYNUCtKFodoFy5TvlgeGFtwrpM1xQWst5VgCAZQT4RRsUaBU+awOUhpE109gqYN/MIF3dFYrNFBQtgnyAPasgADevcEpyf+VrtCn/q0DL9ZRcNLCVjBGA1vL8CctOabm/io4rFf++NXZyPHAMVAAEWi4YX0uFNwBtKLSpMJqBnKDC70J6dPQ7oNmlSGWaGErT8CyCuonhQmC3xkNbEDgaZWlsy8fzDQLXRPmgLY1yFUaTmB4lw4vhyxzQRmt8UfJOQN6TCknXtH4vvwW3U3NexwxVHXBgepBCd5XFagZ7EQJLYfjgpcHKuzKfPBOahoynKdeyKxBY4DvyLDIfQRtyj3jcwzGWeR0Op5a5EyF6Bi8j88vwoueV54zeo7zzaM7JvQhpUtErCYjfl5uHdKaJDudkpZbCqCm0DQRCd9DtU3AalJoptman2TM7hArAKHgwa2E2AwLbQCswXR3fMx5TBcrT8bMR6HCcoudVMqeSUOCnZL4pX8fPiqHi96W0vAsVhGMXniNjF869cK4HtiHrwwta895QrftFc99U8TkYKp4Ty8+Lv4fWeggS91QKP22QHahSrKcxDE3gGhgN4RNoMJoyD2Q+ytpdsn4j8qJ5Gh3Twk+Ur1tz21AxXV5KYXiaysLRGa11P48Qj4UAOCkopa4FrgVYv8bir2/eyD9+/uUM3eZS7bfiSeGlFT27atjjCxAE0Ghy5JWbcYoaL6voOOSRHS1CAF5XGpTCnimjSlVSH2/y5a03HXPvGb/C8+96PV3vLXDkKocH3/CB+Ltn3PNyphbyBEeypOYM1n9zAbcrzXc+/bEl1/jo4hBzfo7Xdt7DXxx9Prd/4XwyU5pb3/WB5beLsc8t85IPvQ3lw71/+P7EN+mYrvvdHG/7X79L14NlnPfMUHzXOqyaj1V2aXanSB8toRbL8jPfR3d3cOilfaRnNYPfm0S5HjSaaC/JWQKCDUPs/WOH7pvT9P94GtV0wfPRtRooAwIfLItdb9+E0dUkdW+GodsamFUPe3we6g1ZPJ6PSqfY9ff9DPUtMnX3IGu/52EXm1jTJVS9KYvD8yDlsOmLU/zrmluPGbs3dE4w41d42pfeSmrG4P43vZ/lGPXKrLfy/NnkhXzz009n7Y1zFLd3AuCUAjL75yAI8PsK+BkLe65GbSRPea2Fm1P0313HmSzTHMjj5UyUp0lPVlHVBqMvH+TeP3g/sA6A985v5NrO3bzogV/De/8QmakG1lyFp332Pv6mf6n3ctwrUzAsPlncwmf+7EXMnmNRP69G37dSmA2NVdP4KUXucBUMhTVVRFsmfk8Oo9xEp0xqIzl8R5E7WsOaWAiZgcLrK3D4bZrMTQUKox5W3cdaaKBtA7PcAD+AIODQrw0R2NC9K8BsatJzTdy8hVX1sedqGKUaaM3UVSPc/r8/EM+xCG8dv5hf7rqLu+rr+dz/fgFT1zTYd/W/h9/mOOCW+YsjL+EpnYf4wrufR99ts7i9Ofa90mHox4rAguyEK89X8VCujzU+D56HzmWonNWPXfbwsiZmPcCseVhTRZkfjg21Oo3tI7zi/Tfyrp++EGfMZvgnHumJKuZ8CVwPbAtdqzPx8i2UNkF+VIR9btxHBZCZFIlrVlzMmUWOvmw93rMW6ftwltyuKWi6EASyFoxQAPV3k79+ltsf2Mz2D9VQro+xUIamiw4ClFJoz4OeLubeq7hs8CDfuuFSendquu6eRc0X5XvThECz+71r2fusf+d4+LPJC7nh40/HLmmcsiY31sCeLKI8n6CQQdsmRqmOqtbx+zopbSvQ6FAYLqQXfHL7iqimS9CZRZsKa2KB6WeuYfrpHr07LO68/q2Hjnvzk8BjIQCOEq0qwdrw2BJora8Hrge45IK0viK7j3fZmsp1i/zgwk/yzLt+i8qtfZg1KK9NY/cMkJ6sYe4fo3imh3JDbdO0KK/pxq6IBpGddHHqTXS5zL1HN8LWYwn8x5lfYm6sk1xe0XnxzJLvrhraw6f3PR36XNJ7nFDT9Y+5xlXZvRQMxYCZ4/k9O/mZcz6NbnXMeUlssfNoA9zzKit+XzAcrp+4ChQYxRobc7Pc3rURbSh8xxBt1zLAdWWhNV0qT13Pfb8vzPOX5n8HuxqQ3XEIXa0CoBwHDMXiGQX2PuuD8CzY9LVr6bvNZOD74+iFJsqywLJQSrH/Vz4kxDwLLr/7V5n7+QCbvuxhTnjoag20WCQxszgf5n+jylM/9VbWft8hs28WFkrQaIDrsjU7ecxzvq5jDDDoM3METoDhreyJXG/lAbgoe4ivZZ7OwjldlNcapOZEI3bP78Mu+SgNqakqRqmGm+8gsKA2qJk5L429KQVAZtbHqfii/c0Xedmrdi+516WZ/bxiz8uYLuXJ5Q067ppHpx2u67kdyC05dzika6Mjc6fRoynka3jpNFZdo7TGbIDR8NCGgZ6Zg1SK2SsGsBpZnHJAYIqm1+hJ4WX7Mese1nyVRm+aC4f3srPzLLysgQo0RsbCLDZQtQbU6uhqDeUP4Z5bpTGewaorzIaF8jRGM0DVXWFoxRLl9WtWHNvX9v6EEdMHRvmsAqZSS77fZOdZaGT4wI6r6AutQLPcJD3sUtzQIdaZckjP+5g1hdnwwHWFNmDsChOrYuEsQn4sAGysnhTOootZczFnNZOXpPndrqP87gs/AkD59XUu/fAfs+FrYE7Momt1qNXxU4rfeP4P+PS3rhSrxzMxfI3ZdDCagaxP16V+RYldl30K/2kB517/e3TtDuj5/kFRRnwf7ft4PWv4/ObPw+abefn25/Lg9AD9Hxsiu+MQNBoETbH29IYhfnbhpwB47+t3APCUd1zH4FcWRQhrDb7Phy77xIrjG+Hanlv4as/lNDsV9brCzaYxNqXJzHhYNR+z4qJTFvg2lU15xp7vQaCw5iysikVpTQ92VWNXAjIzLpbWzJ8FZ2wdZ/To+hPe+2TwWAiA24FtSqlNCON/FfAbJ/rBzlIvr3jfn1Aoazgfvl3roVRNYXpgCR9LmM4Ghb0WXgbcgo5NaK0UVlNj+KE9Z5porXjVgavJmU3u+th5lDaBWVcM/7TJlqaPM17kyIFeXtNzJZd2HOJfv3wNVlWx4edN6r02nQ8WMefKGI7NxTt+nUothWX59OarTP90GN8Rs9BZVAz+vEl5xI6fadwr8+XyWfzTHc+BmZS4t0oGg3e4HNpo841qmvcdeg77fr6OwiHFwkVNhr9tgYLuBxZhscS3v3EpI1MNTDdAuQGGG6CqDZnQrgdBQGC3hE5xk4ldNMjdZaGNkKmahoyF2Rrvd1/9Wf68+Jv0/8xBKQWGQlkmGEsZ8W9tuI337L2GIGUR/1wZqGXndZtZ/JEGbt4m49hLzOZ/veEFfPasGS7oO8q69Dyf2fMU/Ps6xIUArP+ZT3mN5m+mz+HK/C4Avlc6m898/5ewSgrDUzhFGLq/ycI2R1wfPlgNMeENVxguoUmfWvQJbEWjaGA0tbgxPC3/3CA2obemJ9n0tWsxagZB3qfvJxYdo016Mya5fTPoYgndOcRl3/t9nrFtL0/v3Edd27zv9mfjjDooX2FVYN3uKXKbBigH3awZ87ArHtZCnSBloSp10Sh9H2UocStaCm0ozKYoLGb4HCrQog36mh0/OpOOWU16zsWsuBhVF9VoguuhPQ+tNb33ecwYWXJTHoarcRaacr1KQ6w730drjVOET5V6SSsXW3l8cvJy7tixjexY+AI09E836dhS5oMLa/i///UScoeh3qcY3OGybaGBUa7A9BxGdyfOD4awKxqzqUktBKTmGhg1D6NUE9p8HwWk5kSLtarixpCHbLmEtG1hl5f6XvJGmvqwR5CxMC1LrE7TpPfeBp+4/XI6xhV2VZOe8zEbGmfRRXkBZqmO9jxsW5iBqQye/sJ7+Ik6n56fpcDzQYtfTlutufvlrTfBVrjkW9eRcxx0yPyF1mOVufJaGLJtsQAME5TiX448h2dvuxGAKb/Cu6au4Ms7niI/8BW5UYvCpKbeD2Yd7JqMhxEJ68gtphRWxccoWmhLY7jiXjQbwuOsuriOUIr0nGL3vmF6xx59As8pzwICUEpdA7wXMIGPaa3/94nO70wP6cu7fxWVy9LY0IObs7DLHvZCHeX64Hqopouu1ESzHOgFx0bbpjBErcX14Qcy8ZtN8H0O/v455I5ouh+sYhYbGItlYYj1hmiztoV35noC28CquBjzZTAMVKkiZqppooslVEeBxYsGyUw2sKfLYJnCgJVC1cUs1/U6eqif2oYCtR6LjgM1rIU6yvVC2nxxzbguk6/Yjl3W9Nw5JyZx0wU7lMWuh67X5XNvN6pYRocaP4B2vSX+/eblZ3Hzf34UgKf/0e+SmXFJ3XsYXRYrQ5kmmCbVy7fy/Y98GIBNX/9t+n9s0f+DowTTs/E56IADH17PA7/0n/zB2KXc8tFL6Bj1yN0/hZ5fQIfuB5XN4nzR4CvbbmTer3LxN97CmpsUHQ/Mi4lcqaI9D5XNULpyK5nxOmbNxetMiZZadTEXyvKsrou7bQR7uow7VBBmUndRdTdkZAG63kCZBv5wHwDmYgUazVCrC325gSx+lU6L6yCblnnjyfyJNDaAoFJl71+fS9/dmp6fTcTjqUtl+ewKk1WDfTTXdZM6NIffW8CoNkUL932ZG5UqulxBjQyiPD/+vW66YlWF99Oeh3JsGOyPGXkEHb5XIqbS3YE72CFuzMjdoLW4L0ItFsDo75WxqdVaz6aMmPETiJZae9Y5ZI5W8HMORsPDqISWhB+05lRPJ3t/s4eN36iJm6ZaFwZomehaKMQaTRjqI8ilMKcX0dW6MFXPg0DLPUPaVCGPt3kYa67SmvuBxPZ0oym/M02CkX5u+IZo2a8fvYJb9m9h+HMOhbsn0JUaui73Vn096LQjrs8gkPUNrfEN5+XBP72AB64Va3jbf17HwB2arh8ekLUZzd0Na/jmdz4PiHvmSw9cyKYPKez7j0CjEV9bbV4fn7cY1Pj13a+g8Q8jZHbsl/VqKDBNFq/eRnrOI7DEFWaVm6hqQ+ZdJIi7OwiyDuZcWSy4IGjFQyLXVOCj0mmC/i60UhhNceUqz5d35bpgWTJ+fV24fXnsiUVu3P0Pd2itLzkRfz0RHhMB8HDRaffry3MvQeWy6J5OVLkqJmy9IZMFZME1XZRptCZcOMmPgVIo00T1dguzj/zhblMYHci1Ug7KtsGyxKVSD6N40UTu7EA3xUXiD3Vjzi9jPOFEjkxMLEuEUSYj93BDjSJ8wfF183l5kW4zvjfKEKHTdIVhpNPgNoXhQ+s5g2Dpo2Yy1C/dgl1sYu4bD11DzfheGAYohdHfy+wzRsjM+GTvPoyuN5aeF55rDPQR5LMYM/MiiAItTCdxf5VOofJ5GZOZIrpckeeJrhXSqAoF6OvC68ri5SzcDov87kWMclWEefg+VCaDrtXEXZV4n7rRlAXi+5BKoTLp8H16SxlP/NrFmokWe+tCLS0Lw5Dz1gyK8K3V5T0pJQvWNFvPkUnHPmGVSomGGPjxtXSjKePhOOhGo6U1RveLrhXIPFG2JbS5CU0zMfYARkcBnc+iKrWljC7JZJWSsWi6Iqgiv/UKUBvXwuyCzHOt47FTtg3pFH53gSBt4RVs3LyJm1Hkx5rY01WU1gRZBy9vowLN1EUZvAyM/KiKfd8huZbfGo/ouVUhL/RWq/J9ZDGG4x/PD8eh9tQtpGZqGKNTKNNAV2uxkIvGUuVyEPjh+C9b80GLPxgDfSxcOkzucA378IzM33DMtOvJu0+nKV2xmXqXQf+tc6hyFV0sy1gn5rjR1Ulz6zBuh01u17SMda0mfCOy6jJplGXJO8ikQ0tDy1yA1nyK+ESccBHIeg8/64RCEI/P8jmbcsK/DZRlorsKqHqTG/a/51EJgNMWBF6CQCamApoDOWylMCo1cTWEGo3KZWGxKJPfsmQRhQMXC4MkDCVmfDhplOMIs2264NgiSExTLIlypcX85YIy8KWS/Bkdr9ZkgkYT3TSgpwtMQ7Q1PwCvSbBYbE3ydKqVAWE78sLr9ZiB4QdoP0CZtKyXMIiqI19jxFiUQqVTcn50zLJITZQxZosxncq2RAtOTiw/oHNvFfPABEGpHF8vuSABtGNjLJRaC1FrYV5KtZhtoAkWi6jFIkFy3E0To7dbnqlUQqUcKhu7SM3WqffZzJ9hULjPQ88txO9MBRrtl0EZBOWK0LJsESjHBs+jsalPfOI1D6PhoaoNjHozft8iPH2MtNHSNMOsElIpYXpA0NeJtk0a6ztJzTUILAOz1EA1mgRdOcyxWXQ2LVZnOC90vd5iJNE7iZibaazM/OO5aIiVFc4Z7RvHztfE36omwjl2SUTfhcIcrWOrUYWxG6qhwIgEfiEv86RYiV0zDPbJb6fn0R15Guu6WNzsUApdyc0hDzPnUrw/S8dBh57bZ/ByNm7Owk8rqkMar9ujui9N16EclCuizSKWZsy4I8GwnFEvZ24gGrUr9C1RRqPnDAIRupElYZpLBWjCHalNg847p0Q4px2UY6NtS6z0arTmfPL7inSUanJNzxMXqG3JeCsVvzv7vkPYOiAIrUyllNBjW5J6ZZqx4NXlisz76Nkj+sOAsZHLgWHIO/K8pUI9mjemKfMjerT+XtyhToyax8yFHdhVLRb+eElcvJmlcZtHgtUhAMIXrYMAe66GCgLRzPJZVK2BMgyCbBpVqqCyovnECy7UwJVpJhhUuPBNeWnaMIQZ+KB9H8POEgz1Y1RlUUeMAaXi4Cq03C1KGZiToQvEsVHdnbiDHSxuyZCbcLFLLnbDRbkeOvCFqWmNymY49NvbxRdah/RsgOFD7khNUgij7A/bkklYFdePbjTF8jBNSQsDVDaDymTwB7vwCg5e2sRZbFIfSFF74wLZD64lO1pE1V2qW3pIzdaFSR6dQtfq6GoVcz69VOOPnjnUVI3+Xo5cM4ifhnU3LKCOTsvYplOiDdfqwqRtS9LZQq1E57OQctCWwa9/7ibq2uGjf/9S7GqAVpDdW6HzXpfKcA9zl/SRne7CdwzMuo9bkGtlx2r4aQvlBSg/AENh1MM4h2Ohmh61PpvAVgRmGrsakJ51W+l8gNH05bcBGNUGWKYwwJTN+POGUVrTs6uBm7OoDpikXjnJzHeHqPdpeu/OkJn1mdtuM7jDpN7vYBd90gdmRLBUa0vTGEPlJJq/yrZaGr9lxe9RWVbM3BQtQR4zw6SgSDLAyFoMhYsq5IWRlythfEujujtpruumMuzQvWMajk7IbxwHd/OQzLEjs5KSmbKpD+bRliKzWEaH6ZBWVWNVDfJHNfUZmyBlkx3XZKc8mJkjtVgiZdvofIb0bIHqgENhT0mYmE4oDknaQ4Ye0RJbZb6/1DINn1NrvUQ4qGj8QqYYM8pozdu2nBsx2ui2w13sfb2MZ+aAQ+6IJjPnk55qYM2WxapqNMStXKuLZyAULtH1lWkKvZHlkBBkcTZvaE1EAknlsi1hUvHlvKRSqhSV89dQ7zUpHGpgFevgi9vaCLP2goVFOdeyUHho16M50kV5fRovpej6rSO8auR23vnVX8Eu9aEC6Nrrw308KqwOARChVkdNzAKh37TelP8NA6NSg0IOvyePOb0IkXmsg5ZUdn2MQl4mRTolGQmFvEjuKBWs3sAf6uXB67Lk9/RglzWGBz331zArTVStCWOSuSLamjCjoKdAsz+H0fBpdjn4KcXsBZqOQwH24dlY0qtMBm25ErDp744zdB5oVnnJZ9/K8EUTjN80jJ+BoVttrJJLdSSNn1L0/HQcerpQ03OgA3ED+aEG1NOFNg0C2+Tws1NkJxSd+6AyYPKcNbu5ae3lOIsZjKxDcYNFd8NBd6XITNmSTeF6MDm91D0SB4pN0JpDr1oXp6e+6Vcvww1Mdr3zXPL7i3J6oyljaVnowR52vbGTX7viVv5u4A4+UVzDf/zVS3lNh2TGXPN3/8haK88lb79OYirTs/TfleWCd9/Fe4Z//rCnxgPNKnNBmm12jQEzxz63zM7mEAWjxjqryEG3i7q2uTg1xRdL5/LtqbPpcOocfP8Z9Nw5x+L2gPSMgVlxcSbLKL+TqXIWZUo+vTZEY8tOi9ByFj2pcwi1viXujqAVTBarLC3KStMVF0AuI27MEMoLhXgmI4I0igfV6yw+90waHQaFox7OQoN6Vwqz7rcEm6mwJ0pUtnRjF12sclOCxYtV6uu7qA3YlNcaFA7mUT1bCFIm9r2HMIsNMBVBbweVi0fI710kNTongrpeh3qdTKNJZjf0/tSIGVicCtloohsNmTsAcwp7FLrSKfB90YqTVkxCsYgt5EyaQ9eeifm0eawbuxj6wm4RpNG45bJiPeUyrQyn0Net0inmrpAMpuyUR/rQgjBmx8KYmhfLw/djNxxaYzQ8XnrBTibqHeyYP4PckTCwWvfiwDhNF6ZmCaLfCcExTfJ+dexeO0Zhaj0kNF1Ud5bFiwZJzXuYDR97dAYdMfPICtCSLFLrNUjPWqhANHflBxg1FyMSgH7o7w/dQ2bFxbczVIcUM3uG+ap1Ifkjiqe+5k667Brf+OzTT2bpnBCrQwAYBti2+NWiCWe0TChtWQQ9eWYu7sApaTqUwkinJEAaBCjbid0EwWAP2jZpdqXI7JlCR74z25JJABhNjw0bp5k8OoKzCOW1Ci+dpfOAQ+7AIkZXp0xMHUbetWb8ih68HAzeXidztIK5WCE910NqdI5gYRFlin+RlIPu6mHsuT0M/3CRp975azimz9GDfWz4kcdkfZie0QA3EwaNFqpkgfpgCuoNlA4L3JQVmoQmuhHIs3oe1swCgzs2kT1SxZyYJ7s3xR27L2Zoeg41JxNvaLGAKlXFvRUGCUWTWZZumTTLg6BVlAK8f83PAPjLt5f4s/5bubE6xH9cc7UIANOguqbAXz33q7yhcwIweUPnBJ9e8Nj+0eswfMUrf+UHvKP/PrysChd0mtShOb49eiY8AgFwlpMNP0lK5hY7zxa7HB87w3YBF8jzlu6DnJc+zIKf413WdpiaZetnM+LmOTqJbjbJTs2yfnoY9KJYHdUGqlonG+WNB6JdxpprsIzhJYexr5Pa2jzOfBNrqijKhueH2n8gFoEpfluvK0t1TZq57Sb993hMv7zOG879CR/88bPIHspT3ejiTKXIjUFxq6b3LkXvVBmrJopHeX0as6HJHwxwFhqkpqt07pZc8tmn9lPaqNh4n0KNjonismkth18IWz6bw77t8BJffQzDhP5umJ6P11scf4AWY/R9cYkl3YbLr5XwyatMhi/+zrs5y8ny4/MD/u5Hv4Ean1pyXb+ng7G/CRj5uwLzZ41Q3KJQHqz5QY3Ma8cxlObgPSMM3tqHU/Ypj1gMfK8uLs6Ujdp/BEKhZB6a5J4/uwDD02ybWcAoV+OaGO0244QBFfnxk/SHdMdurDhmYyxdJ4nn1J4XehjAzxiSjRYVp6bTElQOr5fbOU56vBOjXENVJWspig3G1lImLW46pTCKFVgoYzYLpBZg3Xdq1O1BhqfG+fa552F3NijMr9IsoIeLzvSwvnzja2G+iF7TLyba5IwEk8IA7uyVa5l8jos16dC7U1MYbeAcDn21pSqYBjqbJsg6+BkbbSlSe6fQpZJouEqhbJvm1mHMcpP5czswfI2bU9R7FYXRgPScT+bQAn4hjbYM7NEZgp4CxlyJ6jnDWFUf5+A0ejHMzgh9raOv2YLRgJHvzaEdi9nzCgy/7gDum7sk8yjUrGg0ICWTNWYOOhCp79jhhEn4gk1h2DETgpbbIDIxbTv2z4u7wFhquia1fdM8NsBE6L9tNjF6exh/yQYA3vD7X+fNXYd55f5n8/nNNwNw1gffxO/8+jf58CevYeSWGqPPS3Pza/+RZ93ye/R/NU33rWPohUWhd/Naxq7qZvC2iri6ooyIfJbKWf1UBkypADYVVk3jVDR2ycfLGhiejitaDa9VPWt4IoylGhqpCA8rJQ1fE5gKPyMFX3bVQzUD7NFpgtm5Y1JcVTbTCuCHrp3lgUDlhMpDmKkhi7bF4PB9sG32fHAre5/172z61hvJHHSwi7D28/tjbXrqms1oA5qdivJ5DbSGD13xCd78pTfSd5emvMYgNxGQmfao91ooHwoHKhS35uh8sISx74jcr6eLoCODUarDfLH1MA1Jl2RkgNraApnb9sVBaaOQp3TZBgr3TBFMTLXmUGI+KNPEO3cT9v4JsQ7CbJMoqI7tgNsU6zYIWgkTkdvEto7JBIoC1Xv/5Ax2v+YDbPv+6zjjL+fY/aY1rL1ojKZvMjHTyZov2PzwA9fzT3Ob+cLhi/iLrd9iiz3LH/zP32PmvFRYEBWQmXKxKh5ewSZz35hkKKUcmJiGMCCODoVttIZ0kMgSk+SF1joKlaJlSRAxTDPO5FoSY1y2ppRjo7q75FjTFRdpGCCOk0WCAKOnm6C3A+X6qIWSZIyB0A5QyKGzKYrbO7HqmvR0PXaBur1Z7Lka2rFQNZfFc7uo9RrYZc0d//HWX4AsoMywvvgZf4g9V2f+3A6U1vT+6Ch+fyfKC9CWgduZwi2IT9yqBaRm65hTCxJ4qVTFZZLP467rozacprjBJDBh/Sf3heZ7gF4zyIPXdTJ4iyIz7bKwzaFzv8QTMqOLYoZ6PjqTkhc9tyDaURTFjxh3aB6jFGr9CN+8+QsAvOiXXsrMM0ZQgVQwD35jv/grtRYaoRWYhFhbSAZ1l0zKKDaxUtZIhJUEQJRZEE3WcGJHGQvHBsyN1j3C66mOAkFXgdr6Apvevot/WXsT75m9mL/pv59zf/abrHu7j1Gu4vcWMCcXJPANLQGjFCov2rrKpGMa9Nz8EiEXMaBofI/JZlEKo6dbtD2/tQj1ogToV/zNcqzwvMbQAEEhI+mmlZpYfGEQmcAPU08lsKkSfvpIsBqdHaLV1+vs+7c1PHjFJ/hEsY/XdMzw11Pn8fPfPFsyTNIp+v5jios7RvmXG1/A+vPGmblpDYEJ/Xe7ZH+6dykTitKPqzXRInWwJNsnokGZZphtE8TZKdF4ReMSZcZEWr+u1gjmFo7JQgEwurvRpdLSrCJlSGwqEhT5HChD0iprNXSghQ7bEjoiv3mgUYaSjBXbwujsQNcbBKUS47/3VIrnh9ZFw2D7RyqMvqCT2uYmIzdaGK4msBWd390j7qfInZRKybxuuq01Es71WBiHFnOszUfps9Frz+fi9Rc0Gi0hEE21kOZ4fJJtIOIJEKATfyvTxMjn4oy6qJBMJVtQILEQFSp/kXVlDPbH6cpR8N7rzUm67mJV+FF0244cfmdGEhZqLvXBLLU+ix0ff3QCYHW4gDyP1Og8fncOpxzgFMWf7mdsrFIDo9okdXiKVKStDfXhdWUxDYNgZq4VpJmbx2o0Kcx2sLhxkNqgBsvC3TqMn7XI7DzC2ps6yB4uY86WsMtdWAcnReNxJf0Sy5LsH9tBZbOSLtloolIOwaYRyQWP6gk8j+rmLt509DJu3n8GW705eu6exyhW0XMLBJ6H0dsj51pWuNiM1oT1PPELu1K5SSqF8rxjGdpy8zOJaOEFgZirkPBlLjVdl1x3JSFAqBl7ntQ/FEtkj5pMvjTDrzuvYPxF63jDX96G+kknanov2vMwyhXxp0ZmbCrVym3uLEhg2w/Qjt1azKZBsG09xc050rMu6Z2HRYtdM4iankOXKxi9PeJLr9bR2TQ6bVPeWKDRYeKnYPCmI2L9lWvic01mDVlWiwbHDlNuW0JUOTY67bTGIfTb6+4CxsSsBPIjph+5z6L00VBYPvD3a9j/vI+y+Tv/k/6vp7jis79DaZ3JP6YhO66ZvlZzzdMPsutPz2Hf/+1j58C5bL6ryuzuEdbcUcLYM4peFiz1zt2EfWAS3d2B6shLbUG5GjNllc3I59CS9Ie6MSfml2i5yfepTBMdWpEqCBMi3IRrJwF/ejr+vFwljP+O6lOS3wX+iteM4sPabRJUW/GQoff+hKFl1153xwr0LD+QuMYxSNRSnEid9edXfvaYluMssRP/xl/xusuvpT1vyTOoVAqdTklMo1SVjEXfx5oJM3v8MJhsO+KO7MzT7HTIHJY6DTvriEX8KLE6BAAK5hexqnXyVVfStgBn/6RoG4ncW7RGTcxgFzNxQEmfvZkgZWHtGpXc42qVdZ8uQzpFsFikOrKOwIR0pUrupvvAMAh8H2N2XrIhkn5AQ1wVqq+H6WcMkZ73yf/kABiKen+azJgP2bS4yz2P8rDF7ZPrSd2WB2MeDhwliHzGROlh4XR2RKMvX74RL23QdedMuKCbaKqoQk4Yb5RiFvlbI0QWQVJrDvuqJBExrIhpLXFdREjkZk+87gIa3TDy4zr2fB1jtkjQ24ExK+msuumiSyXsClz5tbey4V5XMksch6BcEcaYSoFpUHzmVqYvNlj3nQbK05j1LMZ8GZ2RLCHT70ZXqlRHMiycYZCaSzGyN42u1aVYJpeV95py8HtymCAB1QWPHJB1LPysha5UUWEB3jG54UntK2SORlenZCuFbRlUpSYMtlpDOzYHX95HbYPLWf/Xo7K5C+VrcvccFeESCViI3+XI0DwANzzzX/iNH/wJHXdP0rHDE5djoJm6cjv/uuZWntF7IR3fuo/OMEaUGdmEUayJhZbU/E2T8roM3RNp/I60VItWGqhaXe5rSGKDajTRYVpuoz9Lpu5hWKa4JaNUZsOIBbkKNLXNvaRHF1Zk4G2cHijThKlZlA4IEsWdsWsNULkcKuWgXZejz++jOqTZ8sUUQWeG2qCDVXkEEmsZVoUAGD5rkY4PaRav6yBI2/h9WZQX4IwttBagY0sQ05eybt2QIjFlmhx+TgfNLs22sYKYwq5LUCxhAEZPF107JqBaE8YcMkejoyBBnExGSs4jzTtk3qrWIDvl4cw3RQIHkLt/El0qozo7Yi174IdT8FMLVRtDzy+2UtMMA2VZlK46A7MZkJ6qYxRrqEaT9W/bzSc3fp+n/dl1pBd8xq4wGdih6frxqAxIZCGEKXE68keHvlZ6u6TCMtAEnXnUxLRkVETVlotF0RjD1EHKlZie2MSP8vBtizv/SjJ/Lrr0VThf6qZrl6TCGYYCN3SJ+AH5o028nQ6p6bKMSTqLqtVQuRy6qwBNl9rr5rn5go/x3ObbGLmlQbMrQ+7gBEalhu7pZOEpg6QWfTLjNQaaaZyiK5o8XZizJXH5hD5Va6oYBvEk2KZmZHxNy0KbZlxop7LZlsoVae6BuMSiVEx/3QD1vjS5Sk3cF2HGhW42MVIO+SMabdgEGZvStYuUyhk2fmgQ+74wEyYSmJEL7SP9nLv1TQze3mDwwVEJtof3BTj776d53sdeS+fYGIHrol1JH+z8wX6xdpa5XnWjQfcPDxKUK5iRO833IZORBAdLGqNFFd64HplDC2jHorFlgNREBg6PyzsdkSI3vViktrWPAy8zGPl+P503TNPGKkGihiD6O+k+RWspBG000FqTGwvw0pKY0vfuUe789lnkxozjX/8ksSoEQF4p/nX9f/MbhTdj7T6KlXJin2W8UPp6pHK3WBEfWtONC53W//csXkcavVCEDWskzW56jsVnbGTyqQYb/7uOdcdUrAUbfT2MvmIN2QlNZs4je2ARVaxAWbKKMAyC+QWyt5RbOdy5nFSv+r7488N+PLpSDf2e8jtVyLfSzWwLN6uod1ug02TnywT5LHd+fS2bh7ezdV9VmoH1upSHHTqzaanyK1eI285CqM2rVlEbCGOs1jC0FhfMcobi+1CpxEIBaLmKEjnsuunylL+9jsBRLJzjceCdH+AV+57Drm+cQWYyx8C39sf+5dTdBxi4PXSlODZjL1wDxhrKT6/ylgtv5pvPO5/89X28ovNPcC/Q3PyfH+Upf3sdObeJDkxUpcbk07r53K++nx6jySY7zzumz+bWuY08OD6AnkhjVRReTqNtjVUySM0prJq0NI5bLBvgLIiv2GxKa2LDRdoyZyQrw2hCs1OySewHRjHH58hNi1Wl/QAVuPHY6kaTvq/uoi/0J3f/8yZ6vQBn/3SrFcOyKueObz9A4VuiUOh0SgKkno/K2mJVzM1jTk6T1NGWBCGjwrC1w1I/Uq5K3CF65309MD4FKUfeuVISV+koyHvzfJhbRPk+6VKBxYsH6fBE+XjgLT0M/dCg+xv30+g02fDfAXa5sTSzp43TiqjOB1hqwSbcttoPxJ3peZTXGHznun/gRfN/yrDpUd/QpG+nefwbnCRWhQDYvbeX1zz/dVjjh6QCspLolhkOSG1rH3Nn2fTuzOPM1zEn5mOmqEfHsLs6CTyPXW/qoOcuk8EfG3TcP09g9WBPFpf6Bmt1Ovf7pBak35C0nZB+IcpxZNG7Lgz2oSbFTROsH8CcWiSo1yGK4MOSQhRMEz3ch6o1pYq0WqPnht1xIFnX6ijTZMP10yLgmi450+DsO3OtlLJ8mO5oW+LDTXQyjN1K41MtZj4nrghdSxTFhFpEXDoPLQtgeT47MPCpe0BrBs7dwmev7uaBqUHu+wOxCp7/wGsg0NIGwzBQU1KnoWybF/72LfyfwXu49Oev5D3fu4YzK7vJfv8BskFAavEcXvOMKxm4dQHtiisCz2fzl2u8Zu4tuOdVeOqGQ9z6o7OwaopMFVJz0kXTzSu0MsLsoACzGeBmpA+/l1IYvsaqyxs165JjbbiSLRQ4Ct+W/vHpOYVVakh2RqPZemat0VG7EKVaRXdaSyOzO/aimy4BYWuGZf2XYkSCNWrvoHWr6tZvvYdo/KU4MQyOKolPFc/ro95l0L27jj1biRMRvM4M5kJaskVcT1yWfT34PR0Y5RrML6LL9ThVOjdawOvN0+hN0bHLIrUoCkhurCG9lRrLKm3bOL1YVsR2TG1OuF4j9Dzo8twdv0P/AZfJ/znM2fWZY1y/jwSrIwsoNaifPvhqgvmFY/uqRNH9kUGqZ/SSnqhKpefYdCv4A6IZ1erUz1lLanReFkiiSGR5gCyq+mtcuo1Gt0XnjnEJVE7PSUpprS7ZJ9VQM8vnwurBpf70JZWOhPm/hsI9Yw3OnjGCUllcFG6itH85krnTYQsHFfn3m25Lc0vGA44XFF5+TjKAnEwfPc5vVS4HhmLiV7ZQvbpMsDfP7td9gBm/wsve8sd03jEeu5kaF29l4rIUG/5rGqbC9r1RiwTblkph00TZNrpcbmVYBFospdB3LW0yrFbTMFiRWamUEwfNdb0OthM3DFuSzRFWcwJLs6Gi79MpGQtDsniSCy3qPRRZl2q5qb48DdQwqD77XPyUIn+gjLHncCvTSmsJOIdZHkZfDzqbpj6cp9lpkZr3SO+eiFONZb6GY2BZcTYSIHEpx1k5WyqyPh0bBvq4+PO7uSR3gL9932tY84V94irzfYJavdWnpo3TCmU7cZo3hHwklUIVwtbjjSa6I0fx7B467p8TxUBr8Q70dMn+AIbiO7f+zRM/CyhI2ejOPMwvQH9o+kZ9PwCjkKe6uYfMkTLGYkW02zAXOW6KtVCMg6y1Lb1kd9aXpJEBUmy2fYNkTjSauNvXyEYVjiH1Bo4tpeJZaeYWhNo1hMHcJCOx7ZABKCrP2C5tGA6OobJpdLWOPbEoLiwjkU4WMY1Q+EQNpeKqwyBo9Z4Js4Skg+Fapq7sJ3/UI/uT3S3TMGw0taQ/TcTwkw24QP4+HvNPFIPpahWVzzH0uV3wBYV71np4HfSZOcauhsv/6jDf/5fL6P/SfaR2jrJhR0OeT4UMKPI/zy+KEB3sR1umFLiVyiIsPQ9K5RYTi8r6YakpvAza91uVpGHedRycTTzLkuuGx1Q6JRW7YYM7LCXN6KJrR1aAp5f0ckky2plfv4CuvXWsOx6MKzeVZfHiv7+ZP+3Zx28f/iV+8N3zKRyAwS/sAh0w8Ykh5o92MvATk4UXV3jXxV/mT7/0P+i7W+OnDEmfjXoqJf2/0X0T87fVBC+hMIS1CvF6mC/y/Ylt7C30k54PxB3Vdv2sTkTrzrZRvd2SmZZI/VQNV3aT8wOO/MpaLnv1nXz3uxey7eMz7H+rCUrDrz86ElaNBXB5zytkQY8MoOYWJcujo4B2XdHYopTPRJB1pQZTleeey/wZFj0PeGgL8j/YE2s9Kpfj4O9sY80Pajh7xli4chOd392zhAEvERhJBpJJiwYfLnpVyIcMXvHA/9lCxwM2Ix++G//Cbdhj8wTjk8Kgt22QvPHJOchm0MWSaIbptLi6BvthodjqQZR0z4RMfPo3L2DHOz7A+be9mrW/PSP9kDxxCWnPj9tHx90wl3cXdFv+7mOeDUSYuW7MTFRHgWB+Ic4iUj1d+N0FNl+/j/ev+Rmbv/g7nPmOPfIMpbK4q7JZdCbFgVcNkpqH4e/NYczMU7l4Pbmfj0I6RXNtD4tbM/TfsF9cblH+MyzNiFnO/M1lvs7jlecvf0YZlFj7DjqyqLGZVnHgSlWs0e+XmeTKsvji/d/hnJuu48w/2J3IlVeM/fYF3P2293NPs875TppX7n825ZdqMEy+ebfsSPeJYh/npMb4+yPXcORDW+m9JSyaS1qFywV3UiCEjcKA+DdGZ0fYFt0Size0wCTBwZdgcxQ7Qqyh46WBtvH4QqVSmEMD8n5dT3hUog9ZjDA4XP9KH98756sAXP26N1JaY5OZ8/nxV972C1AIZvbpy7IvBogDI9rzWm1lo/S1lboAJhdqJE1TDsWrtnL0eQFnv31UimpyWWk53NkRmld58dOXyiu3mY2unWzalcgzTxZUGYP9EAQEUzNi8kc+YdvGu3ArbodFdu98nN6qF4uSNlmrh9lNekkK3xKGHRZCNTf3Y83W4MBh1PAAqlyVrp7JpmRJulcao+NArR2GhSKLz9qK//pZ5u7uZ+vHJiX9MgyIoxT1K87m6DMtNtxQx75zH3r9CMZ8kdo5Ixy5ymbbR8aFyTZcjj6/n+J5sslN17cfBKBx8Wbe85H3c0d9A+/ddTXODZ0sbNekpw16dvloU3z6qdkGRt0TX3cEx6Y+nMcuu5i7DgndPd2hWVyNW2urTEbGMqwrwPXQ41NL5hWQyBYKYga7pIvj8jEE/Au2Yc1V0GOTTL36XAZ+No8aHYdUitqF67GLLsoPsGZKVM/oRys4+kyLzn1QOOxhuIG4J+cWWspEJt1K/Uu6ctYMoQ8ekfTVSkXcZ1EtiR+Ilaqk4y2IdWQM9IlS0mjEKcT+Bduw9o/Loy4stl1AqwRGNov3lO0EtkFq56goJcecZMTzcfyNF1LaHNB9r6Jw2MWq+ljzNb698++e+C6gZJZFMidfl8otn/JyzS6pLUUBzqYU/GjXpeN7e+jckScolak/4ywOvcjkrPeMEUxOS0vlvu64++aSayb85FGXP+15qOEB0bLqy7IpbJtgcrpFc7KttO9j/Xw3duju0SGt+H6rvW3Uwjd6nhU04WB6Bmt6pjVckzNxUNjo75V21tXaMWPyUIw/ov+szx3iv+67kA2f8vmtTbfwrsXn86r//iFHmr3c8opzUaUyWmvSP97Flh+Fvu2OAk//9F28pOMuXnv36/D3daFLFVRY6du1r5viOYqunXPxrVJ3HeAN73wLc5c1SeebqLxs9q5N8DIKFUCQV1hVS1xGriOdYV0PraTNg+FrzIiZp2zJ8w8358A0pQOoZaJdKQRTDbHukrEblclIhavjhIwyKpwLu1CGmn3cAyiEeeeDsnd3ocAdfyt7P5//njex9qP3kbl1D1FlaaA1nf9e4yvbbuQpf3sd/Z+6WzT4lNNKKQ0CUIrFq7dhlwMyR0uyj6/ng2VS3txJbnImTv3VXgM10Mv8U/rIH25glpuo3QeXdNecumqEer9i3Ud3xS7K+mCKxcu2MrCjhnV78YSFUm08jggC7J37pbL6JFx0I5+S3fLibrMPVf1+klgdAiCJJNNa9lnlsq0+2tEuS1G5ejq9tGinFsYAgoD00TLrv1VAFyV7R9cbsH+UgLDYImvFwVaVTrW23ls7KN1Jy7KzUdxfJ9mXPak1JqDWDot5fvDI0iCu67aqNKM4RyTgHophhz5flc2Ir7tWY887u+DAGra974D4k5e7S05ivL98+yV03meRueNBvnTleWwzJ/jbv/5V7n3Zv/CVp1/N3Hn9bP1MGePAWJhWKgVYX/zw1fDbsOOST3PlJ6+DRkOstqZL7pY9nPWjQNJnox4tWjP4+V0MfNrDyGWBqXgjGb1c644087CdMlqTPRD65aPzDtWPmSc6yqEHODzW6lcfxQCqsnWhMg3Z0UvrVrDaMAhCjXpFyyoeM58PLqxhzsvTvduL52OSlokPb+Ly3/xVeu+rtbK3alH7cUvSO5VBYX8ZP+9I988oQ6lYIn97XbLPpmdbdEzN0n23iVosS+Wo1nG7DZou/T+aoLmuG910pT1BrUb+pwdxFtbi7BnDXwXWfhsCrbXUHyWRyPzRvo9aM4SqNQhm51D5nPQ+WiyJqxzituOPBqvDBZQa1JfnfjkM6mZaTbmSPvhCnsqlG7HLnmzfeGBMpGB/b7yFna5URZtzvaVSdSXmGjbDqj7zLGq9JoXDTdL7pwlm5zFyWYJiSVpJO440E1uWRaLSabQftFJWl/nUiy88h8K+krSirTXQ84u4527EfmAUlcsSdOZg32GMrs4W04mCoSvRa5qy2DvylM/uY367xbr/nmbx3B4CS9H97d2SZppKHTt+DwGjX7ZaDObmRbvP5eJW2Aev3cb9b5aU0Gf/1htI3XdYhEDoX1aZDBO/vInBH0yjFkvMX7UJNHTsLaF2j8LmtbDnkAS+1wzRXNOJc8degi3rWDyrQPddc60MoqhIr7sLXalIBfiaIVS1Lm6ok3ye5VCOQ7BtHfXBLLl7xghmZsMHb1leUd/+6HwRcsd3l6hCQarOgbE3XoBd0Qx+65D0dU/67iH+bAwN4PfkKW7Js7jZIDeu6f3CPSu7HiNEyQbJrLiQdmNogMaGHpqdFoW7JwmmZlrvPbJkTVPckrV6OwawinBMFlA6BYP9eL05UAp7fAG/WzaRUffvF7ev64mC098LCyWUaXDD+L898WMA2cF1+pnZXyOYnqX2zHPI3TkqTDFcOCqXw98yLJWhoQYXZfgkFy6GwZ63n0uQDTjzb/aKWyTp0zVNCdKOzoBpMHf5CD0/OQoge5y6TRFCjrNUa1/GeFQ+F+f0Y5qUrjqDjtuPSBrryKA0kVMK/IDpl52Jl1UM/XiBZm+G1Oi89MdPp2Q/3kxazMBkr5OVNM+EdtC4dBtm3cfauX/lrp8Pl1GGLSZKzzsbs6FJzTUwd+6Xy/X3svvaYawtZQrZOuf1jnPv+8+j9xsPxmmURi4rwd57xmQnpkAz9oIRzKbGKWu6bngALAt/8wg3fuU/ef3oFdz+lfPo2+mChszRstRONJoS/A97MxGEO8FBy+RN1DrEzb6ibSOTjbyQnj/4vgjzsEeRrjdEA4ewuK8iabfpVPxdvGfCQwWbwznVe5PNJzd+n3P+9U04T5tjYbyDs/5Ckg+M/l4WP2TT+PwgpQ2K7EWzFK7vJHO0glFtoI+Mr5imrBxbBGDY0oFUCmVZsSBUmQze2Ruw98smMFGzwdh9FbkxCwUR1kDQaLRjAKsEyrLi9awcBwb7CDoymEel7ijeGtY0WntMZ9Iy36OtVDs7uGHfu5/4MQB70YW0CKLM4ZL09y6WWvnchsLcc4Sg3jhGQ17uXunaBXPPdGlcsBEVaJx7D8fMVTkOo8/Lsu47/dizFRbOMOj+bmOp22D5NePq4V5pIVCpxv527fuofI4jLwzY0Bghe2uNxrpu0vUmwfQM2Db9X3tQ8uDrdZw9HtowwirdaiuvNxkDSGY4JREEsZvBuW13fP+YzsR5Dxth0NlLK45e4zPwgywLLzqX3p2arrtn2fr/iQ973/Wb+ehFt7Dp8rPpu6WD6X+xmT7UzcDPTBZfXGbLH3gwWyVoNCivHya9fZH0p6RyFWVgHprk5Xufy5e33sTUm2/gaTe8BdU06Ly/G7uiyY27pGZq0p+/Lhtve4U0fs5OvBwILCkGM5qhm0iDWXMJHFN2YwvnDIHGGp+XsWo0WmMTWnCSNOCgwhbaGMbSDKoTIbqW7zP/6h6eu+51WE+BOy/9LP+2sI5vps6WeaQUnz37E9z4p1v5zJuuwfmchql9opEbBnMvP5/8WJPUkUXZsSrMxqqcP0z2Rw9Cd6cEjcsVgjM3Mn/1BrrvL6KKNaz7D4VdLSX5YfT3zsNwYf0XDkubi2KJ2RduZfqSgMIBk5GP7GwLgFUEo7urtXvc9CxqTAr+WicYcaxPa42/dUSK+nYdDF3WJ+gSfJJYFRZAp9GrL8v98tJgbxC0iqyizZWXu0iSJnwqJTn8jg22w8w1W5l7fo3tfzzecrEg6VeAnBv+ZkWTO5CNoYNSGbQmOH8r5nwVfeiInBf52kMhgCfbuMV7i0bnJAOJSX9/+IxLNiEP3VLH0z6jYhFdrYZ+5FTsD9S1Wuv3y4KXJw3TxOgogGlyyTcO8o7++7jmOa+EyRlxq21ey+wFnfTdNgMTM/hfzHHjWV9n2yev44wPjKEXii2fNMRaexDt5WxbsetMYi1uXKQEHL+rZbJiMoElczcZAE8eizK3Mmmx3MqV0FW4wrkr4XiZVCsIaaOrE93TKW1FAF0uo/0AI5eluX0EZ/+0KAYRbJvd//85pNeVsH7YSc+DLqnpKsaRaejrhrGp2KLRtRpqywbGru6le3eTzK170K4nmxcF0hJ94jfOoXRljTP+VOpodKOJ3jDM9o/u5vZ3PoWOb+xc0pmzjdMHZTtUXnwRlUGD4W8dkVgPHDP3l6eiR/HJ6Pi3a5984ruAOo1efVn6mpW/TAyCyudaOfuu2+r/DRJwrbYCbqqzA5VJE0zNYPT24K7vw7r/UKtpV3htlc1IJklUUBXd03XjBmu6VlvC8KOe+ZhmS6NazgwG+yUukUlJDGBhEdXVKX3sw2pY3Wy2Wk+scI1jYMtm9qqrU1JZa7UW03SXCZ1HgtBvrCwLhvpxBwrMnpvhGW/cwV1/dxH5Hx8QaypiqoU8wboBzJmiuLNMEwZ6Y0tGj01i9PcSTEzJxhmZDNXz15K5a7TV1Ozh0HqSaa0r/U5lMyjbbm08Dy1hvVx4nMz1z9ws7crvO9DqT59IFT78yU382pY7+c7fXkH+xp0tV6Xvo3JZsQrDgi9l2yIIo71pVxqTpGUYxhiiICFIiieBNPeLM0ui8zLSbVV7XtsCWCVQtiP7E1iWrKmo5XiY2KKbTannCAJp7tjZIem9TbdVXQ58u/zxJ74LCDOhuYZFSTESGr5KpSQNz/Ok973WLQshqrBNpyQ4WypLGmkQoFM2WnFsYFgpWZhmWHkZ5e+HzCBubQDHpGce435ZrhGGe74qOuVvZaDnFuS6iRQunUwDfSiEXSUpV1q7Mi3HI2X+gD5jo2ymPjuPHp/COjJOL2fwR/3f45//l2b3/9iMmqZVl9F0UffvR6dS6LM3ox48hB6fAq2pPPdcmpf00XGwjjkxJe/IdnjpP32H67r2cMVf/gG9dy0QZGzMAxPxbmgqmgvA8o1zlvQ6Sn4X+7wT8QCQXuo6EK2/XGmlQMYbh4TBWtMUJrls34AToTaco7zWYmB/mj1/soXshGLtN6fRR8Qnb/6ok+omB2dR9raI8vSDQgY82ejeWCgTTM+2NguKkIwBJBZ7/OxKUlhVsSwWahS8zmTEb5x2UGG8w+goSAzEth5645w2Hj/oYIkyqtIpidd4YXGkDje6Crsb0Az3Sg7rQZSS6nbKx7n+SWJ1WADOgL4s82Jxu/T1yK5FSTdG2KMmlpQrIWSi5RdeQL3LoO9zd68YGFX5XNwi2Dt7gwRSV8rDjfzxMTOxV3atGAZGZ4dolq4ri3C5W+kE9D4smObS9MRkjcQjzJAB4iCwymZQhoGuNyg96wwKu+ZhajauXH7wj9bz0mfdxk/e/VS6v7ufo6/eSschn8K90wRdOdSug61dyUYG8DrTsq3mzGxMe+NpZzB9cYquPT6FXXPS235yVgLwSQGwnMFHiAK9SYa/POMmqg2AVkZYos/SMXPINKXvU6nUct+dzPsL7zv52guoDsPAz32mL7BAwaZ/fkCYccph3x+ewdrvN7EqLn7GIrV3SjLLlIJtGzBmFsUCqEnqZ2TlxYWFURfYQoFg4xDGYlWEdKW6pBWEd+l2vIxF5me740C22riW6sYOnAUXY8cDbQtglSAZBMY0hZlb1jHxyPj8dAqVz0vcMCoMNBQ3zlz/C2ABJLf6K5ZbGzYng72VyorMd0nwFOj46UHyQ71xPrqu1Zb+LswQ0Z6HNVVc0kRsCcIYxIrfL/Mf60AYl3ZhyVZACbrijKXl2SwPh3lHmR3RdZLC+3gujOUuoZWymsIdtHSp1Q7bWfAIcikMZQABwfwCHXs38Iev/CE3DV9Gt+8zeFuF0oYMyvMx9h6R30a7cR0exzzgocOaBQBlGmTuO8ranxTjcQOWFCfpRx/XWhmGQdwae3m7Ba2XpvpGgvZE7yXx3eDH745dMrNnDeKUkDH3Zc+Brj3gZU1S9x6WHdSiH2qNsSC7yyltxeOgXQ8WFuVzwsrT9Trm2Kx0wK3V4xhZ9I6d3eNYvV1LhJweHSOdsTEOTxEEp1/ZayMBu5XcENUsLUFyDobbj8b9zR6NwpfA6rAArD59mfPCR/bj5QHhEKoQ9k1fqcQ6+Vs4sWBZ6fuTQZKJJPvdrHSvh3uPR0PXCaAymfhz1Da6/PzzcN48jvMnedT4zDEbgscuiahrZiIGE21qEzExlU5x4PfPJDUnlY3xvruJNgjHHffk+z1R3vwjwaO1oJLtQkD8/OnUknbcKp1qMeZlisGK9z4Zmpa7S5cFDZePabsOYPVAWZZsF+v7rRoYWDr3k3uCh27LJVXqPPogsPHQpzwOOBkZZBjHZm5Aa8InJ74hefVLcuuX/zZaGCstsuQ1Hylj8H15cdH/rnv8ez3cezwaulZCNGaeFxa5hWmSpknHjqNM3bCW6tr80hYWSuFdtA2jo9DanhJk0pqm+Cz9QM4PA5u63mD9DWW69rrIHgm6lckTWwlmbB6rcNIrO/G3bcV++/ieK82LEzznMXgEY6k2rZN/jkP9ynNQ69fIOIQL8/Abz8G7dHssVCNh12pFnbBCVqAzylZbQq9htFpCJ6rKY5oy6bhxYix4LCv0L+cf9jO28dhCLxaXbhm7vIo/kSEX96uKPp8irA4XUBLH03xWaLew4s87Cq0K3Ycy4R9NxsxxCTCOr7k+Wk3zePd4tEiknkYpm0Zfj2zePjfP2vfPAXD02gsJLFhz8yJq90GsYp1goBt1uNkKaEV9eqI4yDJazT1HyNQbLdfasuc46an9SN7bKRyzxfN6CSzoKVbI7Jd+7do0JfsLGP5JBaMR7ilQKIj5rjWKZT2GVnI3RXULJ6J5hbGNC4aSxzxPXIZR1lUbqxcrWYPJd5mMLZ6s0vMQWB0CYKUWzMtxkgs+riA+wUCueN9ThZP0G5+ye5wKYRAxpFjDNuJAvDE0QOWsfrI/3s3grRUmn5ZDp0zZlnLfYdHilZLgerXWupZhLGmmF6Uv6rWDeIUU9q7DrdgPLDnnIXPykzieYvBIAuwPw7fa8fV7wtsEMBsKLtuWjeY9D/Pn0gFVE9ZvhAL2mBjHCWJQKx1bkrCw/JyoruJEv2lj9eJEc+94BaKPEqtDAJxKPBzmeIoH86TvcSpf5Cm4TpxqmGCkRm+3uG9cj+nzbQYbW0n9dBcjdyfO6e9FZ9MwMSPMPNJKI0YdNdBLutT2H8FO1F7E2T4R83+4OFVC/CEC5ccgGW/q7JAMnmZTgsnLSTwRAz7Vi3ql68UxqMdhvrfx2CBKKohwiubML54ASOJUa8qPFCuY+KsJyb0I4jRIpwd3bQ/WnXtY/6+LUvg2PICqN8VN5LroShV/sAtzWrbQjBFl9xwv7zwsaFmCkxmTx2vcHuZ94hS+RCO4qHlXnNJ8vOue6mdaac6vsvnWxurB6ggCH6fU/5QhmXVxinxnx1z/RHiiLEClpBYglZJCsJ/vloKjtUOSJppNsXjZWmk/gKSuGfftBz+QFhJRK4rwf5VJr9ieWmUyEjzOZuL7LQmMRoiut/xfZGU8nHd5qt97gt5gclo0f9+XgsVMWrbFzGfjrTuVYz82cw+WFiMmkcyoauOJDcOQBItT/D5XhwXwWKWirqQBPRbM+ETXtO3Y/7vq4bf23DUGpEW0nlvA68lh5NZh7D1Mx+g4QbiVo8rnCEb6UUemxCpYlpG1Ym5zFGhOnHdcnMp39VDXergJAclzE0WCUTBclysQtZ3mMaxvgGMz4ZZ/dzqt3zZODaJEDTil7/MhxYlS6mNKqSml1L2JYz1KqZuUUnvC/7vD40op9T6l1F6l1D1KqYtPCZWPFKth0idTuVY7TDPWxoPZeWmf4HmYd+1B3b9fms+lWmmGulTGOHBEWl5kWzUEsZaSTNUMoSxL9huwLGGciWKYY/BwtfxHg0fxjpa0r0haJ6tF+14N66CNR4coBnCKXXonM0P/A3jBsmN/Dtystd4G3Bz+DfBCYFv471rgA6eGzCcwVgsTOBn4vgQsA+lgiR+IS2PDmti1sXjlJtnzgCjvPC2V27XE7lzRBF0euELiArpSiQvNTsh4V7P/OvFedb2xpBlfJOSM/t5WXcTD3antFNB1wmNtPLHwGK2Dh5wZWusfAstTG14KfDz8/HHgZYnjn9CCnwFdSqnhU0TrExOrlYGthETFsq7XJbDZaEg/IEMRTEzReeMD6MNjgDA+7YUdLE+WyUSxgZVSdSMaVvq3mrEk80m2FtWlUisA/FCC7lTi8Qg0t/ELg0caAxjUWo+HnyeAwfDzGuBw4rwj4bFxlkEpdS1iJZBWueVft3E6kAgaaj+AZtjXPuo/EmmzyerEqN3Byebiu+7S/konU6+xGnG8uFKylP8kO4u20cZJ4TFoAfOoVSsttfwPO4qrtb5ea32J1voSh9SjJaONR4PjtEeIqkqVbcXbK2KuoJFHfYFO9l4ncoc8kTT/4yEqqnOcJ+4ztLE6sUoKwSaVUsNa6/HQxTMVHj8KrEuctzY81sZqxnG02SiPP25qZttLG1eR2JXrZF0cJ8pYOdHxJwoS8Q/9MCqL22jjIfEYzKVHqp58DXht+Pm1wFcTx18TZgNdBiwmXEVtPBEQuXkSLh3l2FIPoJTUAMQbtiSCtCtp9cfTfqOy9tWkHZ8qWpLP1Wb+bZwqJGtsTiEe0gJQSn0GuAroU0odAf4GeCfweaXUG4BDwCvD078JXAPsBarA608ptW089ljBP5/cNlHPLxzbkOrhavOrUTN+uPQsb/IXXeNkx6aNNh4OHqN6jocUAFrrVx/nq2evcK4G3vxoiWrjNCLeXnGFfjJhiqNuNh/5xvPwi8EUkzGPduZNG48HHoM5tToqgdtYPXiI7pR6OeN/JFrJL0Jl6hOluK+NNk6AVeSEbeMJgeV+yEfCyJ/ozL+NNn5B0LYA2jh5PFQGTxtttPGEQtsCaOOhkWT4UQZPG2208YRH2wJo4+Gh7ftuo41fGLRVuTYeHtrafxtt/MJA6ceqF//DIUKpEvDg6abjJNAHzJxuIk4CbTpPHZ4INEKbzlONJwqd27XWhUf649XiAnpQa33J6SbioaCU2tGm89ThiUDnE4FGaNN5qvFEovPR/L5tz7fRRhttPEnRFgBttNFGG09SrBYBcP3pJuAk0abz1OKJQOcTgUZo03mq8aSgc1UEgdtoo4022nj8sVosgDbaaKONNh5nnHYBoJR6gVLqQaXUXqXUnz/0Lx5TWj6mlJpSSt2bONajlLpJKbUn/L87PK6UUu8L6b5HKXXx40TjOqXU95RS9yul7lNK/eEqpTOtlLpNKXV3SOc7wuOblFK3hvR8TinlhMdT4d97w+83Ph50Jug1lVJ3KqW+vlrpVEodVErtVErdFWV/rML33qWU+qJSapdS6gGl1OWrkMbt4RhG/4pKqbesNjrDe/9RuH7uVUp9JlxXp25uaq1P2z/ABPYBmwEHuBs4+zTScyVwMXBv4tg/AH8efv5z4F3h52uAbwEKuAy49XGicRi4OPxcAHYDZ69COhWQDz/bwK3h/T8PvCo8/kHguvDzm4APhp9fBXzucX73fwx8Gvh6+PeqoxM4CPQtO7ba3vvHgTeGnx2ga7XRuIxeE9nXfMNqoxPZT/0AkEnMydedyrn5uA72Cg94OXBj4u+/AP7iNNO0kaUC4EFgOPw8jNQsAHwIePVK5z3O9H4VeO5qphPIAj8HnoYU11jL3z9wI3B5+NkKz1OPE31rgZuBq4Gvhwt9NdJ5kGMFwKp570BnyLDUaqVxBZqfB/x4NdKJCIDDQE84174OPP9Uzs3T7QKKHjDCkfDYasKgbm1rOQEMhp9PO+2hiXcRol2vOjpDt8pdyJ7RNyHW3oLW2luBlpjO8PtFoPfxoBN4L/A2IOp617tK6dTAt5VSdyilrg2Prab3vgmYBv49dKd9RCmVW2U0LsergM+En1cVnVrro8C7gVFgHJlrd3AK5+bpFgBPKGgRrasibUoplQe+BLxFa11Mfrda6NRa+1rrCxEN+6nAmaeXomOhlHoxMKW1vuN003ISeIbW+mLghcCblVJXJr9cBe/dQlyoH9BaXwRUEFdKjFVAY4zQd/4S4AvLv1sNdIYxiJcignUEyAEvOJX3ON0C4CiwLvH32vDYasKkUmoYIPx/Kjx+2mhXStkI8/+U1vrLq5XOCFrrBeB7iLnapZSKWpAkaYnpDL/vBGYfB/J+CXiJUuog8FnEDfTPq5DOSCNEaz0F/BciVFfTez8CHNFa3xr+/UVEIKwmGpN4IfBzrfVk+Pdqo/M5wAGt9bTW2gW+jMzXUzY3T7cAuB3YFka1HcQc+9pppmk5vga8Nvz8WsTnHh1/TZghcBmwmDAfHzMopRTwUeABrfU/rWI6+5VSXeHnDBKneAARBK84Dp0R/a8AvhtqYY8ptNZ/obVeq7XeiMy/72qtf3O10amUyimlCtFnxHd9L6vovWutJ4DDSqnt4aFnA/evJhqX4dW03D8RPauJzlHgMqVUNlz30Xieurn5eAZcjhPouAbJZNkH/NVppuUziK/NRbSZNyA+tJuBPcB3gJ7wXAX8W0j3TuCSx4nGZyCm6T3AXeG/a1YhnecDd4Z03gu8PTy+GbgN2IuY3qnweDr8e2/4/ebT8P6vopUFtKroDOm5O/x3X7RWVuF7vxDYEb73rwDdq43G8N45RDvuTBxbjXS+A9gVrqH/BFKncm62K4HbaKONNp6kON0uoDbaaKONNk4T2gKgjTbaaONJirYAaKONNtp4kqItANpoo402nqRoC4A22mijjScp2gKgjTbaaONJirYAaKONNtp4kqItANpoo402nqT4f0DLqcwP5hBsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec, genre = next(iter(data_loader))\n",
    "print(f\"spec shape: {spec.shape}\")\n",
    "plt.imshow(spec[0][0], vmin=0, vmax=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the encoder and the decoder\n",
    "\n",
    "Define as `nn.Module`s:\n",
    "- the encoder\n",
    "   - Input: waveform\n",
    "   - Output: latent representation\n",
    "- the decoder\n",
    "   - Input: latent representation\n",
    "   - Ouptput: waveform\n",
    "- an encoder-decoder that chains them both for convenience\n",
    "\n",
    "### Shapes\n",
    "| Stage                  | Shape            | Meaning                      |\n",
    "| ---------------------- | ---------------- | ---------------------------- |\n",
    "| Input `spec`           | [5, 3, 128, 801] | (batch, channel, freq, time) |\n",
    "| Output of `ResNet34Cut`| [5, 256, 8, 51]  | (batch, stack, freq2, time2) |\n",
    "| Intput of LSTM         | [5, 2048, 51]    | (batch, freq3, time3)        |\n",
    "| Output of LSTM         | [5, 2048]        | (batch, component)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resnet utilities\n",
    "\n",
    "Cut the resnet34 architecture to keep more detailed information (and not loose temporal dimension):\n",
    "\n",
    "NB: the `forward` implementation is almost copy-pasted from the torchvision sources, from [here](https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet34).\n",
    "\n",
    "Also, define an inverse module for that architecture to be used in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:37.001226Z",
     "iopub.status.busy": "2022-05-10T09:47:37.000544Z",
     "iopub.status.idle": "2022-05-10T09:47:37.018441Z",
     "shell.execute_reply": "2022-05-10T09:47:37.017966Z",
     "shell.execute_reply.started": "2022-05-10T09:47:37.001185Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet34Cut(imgmodels.ResNet):\n",
    "    def __init__(self, pretrained=True):\n",
    "        resnet = imgmodels.resnet34(pretrained=pretrained)\n",
    "        for k, v in resnet.__dict__.items():\n",
    "            setattr(self, k, v)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.size = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x.size = [batch(B), channels(256), height(8), length(f(L)=N+1)]\n",
    "        # f(L) = ???, at least f(16×n+1) = n+1   for int n\n",
    "        #         ↳ Because of mess of conv & pooling layers in resnet\n",
    "\n",
    "        return x\n",
    "\n",
    "class InverseResNet34Cut(nn.Module):\n",
    "    channels = [256, 128, 64, 64, 3]\n",
    "    kernel_sizes = [(3, 3), (3, 3), (3, 3), (3, 3)]\n",
    "    mid_kernel_size = (3, 3)\n",
    "    num_mid_layers = [2, 1, 1, 0]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = self._make_layer(\n",
    "            in_channels = self.channels[0],\n",
    "            out_channels = self.channels[1],\n",
    "            kernel_size = self.kernel_sizes[0],\n",
    "            output_padding = (1, 0),\n",
    "            num_mid_layers = self.num_mid_layers[0],\n",
    "            mid_kernel_size = self.mid_kernel_size,\n",
    "        )\n",
    "        \n",
    "        self.layer2 = self._make_layer(\n",
    "            in_channels = self.channels[1], \n",
    "            out_channels = self.channels[2],\n",
    "            kernel_size = self.kernel_sizes[1],\n",
    "            num_mid_layers = self.num_mid_layers[1],\n",
    "            mid_kernel_size = self.mid_kernel_size\n",
    "        )\n",
    "        \n",
    "        self.layer3 = self._make_layer(\n",
    "            in_channels = self.channels[2], \n",
    "            out_channels = self.channels[3],\n",
    "            kernel_size = self.kernel_sizes[2],\n",
    "            num_mid_layers =self.num_mid_layers[2],\n",
    "            mid_kernel_size = self.mid_kernel_size\n",
    "        )\n",
    "\n",
    "        self.layer4 = self._make_layer(\n",
    "            in_channels = self.channels[3],\n",
    "            out_channels = self.channels[4],\n",
    "            kernel_size = self.kernel_sizes[3],\n",
    "            num_mid_layers = self.num_mid_layers[3],\n",
    "            mid_kernel_size = self.mid_kernel_size,\n",
    "        )\n",
    "    \n",
    "    def _make_transition_layer(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, int],\n",
    "        padding: Tuple[int, int] = (1, 1),\n",
    "        output_padding: Tuple[int, int] = (1, 0)\n",
    "    ) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=(2, 2),\n",
    "                padding=padding,\n",
    "                output_padding=output_padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.01),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def _make_mid_layer(self, channels: int, kernel_size: int) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(channels, momentum=0.01),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def _make_layer(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, int],\n",
    "        num_mid_layers: int,\n",
    "        mid_kernel_size: Tuple[int, int],\n",
    "        padding: Tuple[int, int] = (1, 1),\n",
    "        output_padding: Tuple[int, int] = (1, 0),\n",
    "    ) -> nn.Module:\n",
    "        l = []\n",
    "        l.append(self._make_transition_layer(\n",
    "            in_channels, out_channels, kernel_size, padding, output_padding\n",
    "        ))\n",
    "        for _ in range(num_mid_layers):\n",
    "            l.append(self._make_mid_layer(out_channels, mid_kernel_size))\n",
    "        \n",
    "        return nn.Sequential(*l)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(f(L))]\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:37.019400Z",
     "iopub.status.busy": "2022-05-10T09:47:37.019236Z",
     "iopub.status.idle": "2022-05-10T09:47:40.235014Z",
     "shell.execute_reply": "2022-05-10T09:47:40.234207Z",
     "shell.execute_reply.started": "2022-05-10T09:47:37.019379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "x.shape = torch.Size([1, 3, 128, 17])\n",
      "y.shape = torch.Size([1, 256, 8, 2])\n",
      "z.shape = torch.Size([1, 3, 128, 17])\n"
     ]
    }
   ],
   "source": [
    "# test shapes are coherent\n",
    "resnet = ResNet34Cut()\n",
    "inverse = InverseResNet34Cut()\n",
    "\n",
    "resnet.to(device)\n",
    "inverse.to(device)\n",
    "x = torch.randn([1, 3, 128, 17]).to(device)\n",
    "print(type(x.to(device)))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "y = resnet(x)\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "z = inverse(y)\n",
    "print(f\"z.shape = {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder and decoder\n",
    "\n",
    "Remark: the LSTM input size is constant, as it is derived from the dimension of the output of the cut resnet model, that is the product between the number of channels (256, fixed by ResNet34Cut) and the height of the output, that evaluates to `post_resnet_height = n_mels / 16 = 8` as long as `n_mels` remains 128.\n",
    "\n",
    "**Recall**\n",
    "In pytorch, a mono-directional LSTM set with `batch_first=True`, having `num_lstm_layers` LSTMs stacked on each other, takes as inputs:\n",
    "- the input sequence `x`, of shape `[batch, sequence, H_out]`\n",
    "- the initial hidden state `h_0`, of shape `[num_lstm_layers, batch, H_out]`\n",
    "- the initial cell state `c_0`, of shape  `[num_lstm_layers, batch, H_out]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:40.236821Z",
     "iopub.status.busy": "2022-05-10T09:47:40.236602Z",
     "iopub.status.idle": "2022-05-10T09:47:40.254506Z",
     "shell.execute_reply": "2022-05-10T09:47:40.253769Z",
     "shell.execute_reply.started": "2022-05-10T09:47:40.236796Z"
    }
   },
   "outputs": [],
   "source": [
    "post_resnet_height: int = int(N_MELS / 16)\n",
    "lstm_input_size: int = 256 * post_resnet_height\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 2048,\n",
    "        num_lstm_layers: int = 1,\n",
    "        output_size: int = 2048\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.resnet = ResNet34Cut(pretrained=True)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.h0 = nn.parameter.Parameter(torch.randn(hidden_size))\n",
    "        self.c0 = nn.parameter.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.init_state = None\n",
    "    \n",
    "    def set_init_state(self, s: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        self.init_state = s\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.resnet(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        length = x.shape[-1]\n",
    "        x = x.view(batch_size, -1, length).swapaxes(1, 2)\n",
    "\n",
    "        # x.shape = [batch(B), length(N+1), height(lstm_input_size=2048)]\n",
    "        if self.init_state is not None:\n",
    "            output, (hn, cn) = self.lstm(x, self.init_state)\n",
    "            self.init_state = None\n",
    "        else:\n",
    "            h0 = self.h0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            c0 = self.c0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # hn.shape = [num_lstm_layers, batch(B), hidden]\n",
    "        x = self.fc(hn.squeeze(0))\n",
    "        \n",
    "        # x.shape = [batch(B), output_size]\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(output_size, hidden_size)\n",
    "\n",
    "        self.const_x = nn.parameter.Parameter(torch.randn(lstm_input_size))\n",
    "        self.const_xr = self.const_x.repeat(1, sequence_length, 1).to(device)\n",
    "        self.c0 = nn.parameter.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.reverse_resnet = InverseResNet34Cut()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def set_output_sequence_length(self, spec_length: int):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output spectrograms of length `length`, \n",
    "        parameterizing the LSTM input.\n",
    "        \"\"\"\n",
    "        length = int((spec_length-1) // 16 + 1)\n",
    "        self.sequence_length = length\n",
    "        self.const_xr = self.const_x.repeat(1, self.sequence_length, 1).to(device)\n",
    "    \n",
    "    def set_output_sequence_length_seconds(self, seconds: float):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output approximately `seconds` seconds spectrograms.\n",
    "        Relies on the MelSpectrogram class used, obviously.\n",
    "\n",
    "        Dumby interpolation from 801 spectrogram length corresponding to ±20s,\n",
    "        hence 40 length is 1s.\n",
    "        \"\"\"\n",
    "        self.set_output_sequence_length(int(40 * seconds))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), output_size]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # x.shape = [batch(B), hidden_size]\n",
    "        lstm_input = self.const_xr.repeat(batch_size, 1, 1)\n",
    "        c0 = self.c0.repeat(batch_size, 1).unsqueeze(0)\n",
    "        x, (hn, cn) = self.lstm(lstm_input, (x.unsqueeze(0), c0))\n",
    "        \n",
    "        # x.shape = [batch(B), length(N+1), hidden_size]\n",
    "        x = x.swapaxes(1, 2).view(batch_size, 256, post_resnet_height, self.sequence_length)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        x = self.reverse_resnet(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(16×N+1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_input_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:44.740524Z",
     "iopub.status.busy": "2022-05-10T09:47:44.739839Z",
     "iopub.status.idle": "2022-05-10T09:47:45.972036Z",
     "shell.execute_reply": "2022-05-10T09:47:45.971439Z",
     "shell.execute_reply.started": "2022-05-10T09:47:44.740496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v.shape = torch.Size([5, 2048])\n",
      "y.shape = torch.Size([5, 3, 128, 817])\n"
     ]
    }
   ],
   "source": [
    "# testing...\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "x = torch.randn([5, 3, 128, 817])\n",
    "decoder.set_output_sequence_length(817)\n",
    "v = encoder(x.to(device))\n",
    "print(f\"v.shape = {v.shape}\")\n",
    "y = decoder(v)\n",
    "print(f\"y.shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training the model\n",
    "\n",
    "### 3.0 Utility for storing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:47:50.368335Z",
     "iopub.status.busy": "2022-05-10T09:47:50.368049Z",
     "iopub.status.idle": "2022-05-10T09:47:50.376413Z",
     "shell.execute_reply": "2022-05-10T09:47:50.375810Z",
     "shell.execute_reply.started": "2022-05-10T09:47:50.368308Z"
    }
   },
   "outputs": [],
   "source": [
    "class LossesHandler(object):\n",
    "    def __init__(self, test_name: str, train_name: str):\n",
    "        self.ftest = open(test_name, \"w\")\n",
    "        self.ftrain = open(train_name, \"w\")\n",
    "        # self.losses[e][i] contains the loss at epoch e for elm i\n",
    "        self.train_losses = [[]]\n",
    "        self.test_losses = [[]]\n",
    "        self.cur_epoch = 0\n",
    "    \n",
    "    def add_test_loss(self, test_loss: float):\n",
    "        self.test_losses[self.cur_epoch].append(test_loss)\n",
    "    \n",
    "    def add_train_loss(self, train_loss: float):\n",
    "        self.train_losses[self.cur_epoch].append(train_loss)\n",
    "    \n",
    "    def next_epoch(self):\n",
    "        self.train_losses.append([])\n",
    "        self.test_losses.append([])\n",
    "        self.cur_epoch += 1\n",
    "    \n",
    "    def write_epoch_losses(self):\n",
    "        self.ftrain.write(\", \".join([str(l) for l in self.train_losses[self.cur_epoch]]) + \"\\n\")\n",
    "        self.ftest.write(\", \".join([str(l) for l in self.test_losses[self.cur_epoch]]) + \"\\n\")\n",
    "    \n",
    "    def write_averages(self, filename: str):\n",
    "        f = open(filename, \"w\")\n",
    "        f.write(\"Epoch number,Average test loss,Average train loss\\n\")\n",
    "        for i, (test, train) in enumerate(zip(self.test_losses, self.train_losses)):\n",
    "            avg_test, avg_train = [sum(x) / len(x) for x in [test, train]]\n",
    "            f.write(f\"{i},{avg_test},{avg_train}\\n\")\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.ftrain.close()\n",
    "        self.ftest.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-10T09:53:47.381630Z",
     "iopub.status.busy": "2022-05-10T09:53:47.381362Z",
     "iopub.status.idle": "2022-05-10T09:55:31.620748Z",
     "shell.execute_reply": "2022-05-10T09:55:31.619863Z",
     "shell.execute_reply.started": "2022-05-10T09:53:47.381602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch with spec length: 801\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "epoch_backup_spacing = 10\n",
    "epoch_display_spacing = 10\n",
    "h = LossesHandler(\"test_losses.csv\", \"train_losses.csv\")\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# spec lengths chosen randomly at each epochs\n",
    "num_choices = int((max_spec_length - min_spec_length) / 16)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "master = master_bar(range(1, num_epochs+1))\n",
    "\n",
    "for epoch in master:\n",
    "    r = int(torch.randint(num_choices, size=(1, 1)))\n",
    "    l = min_spec_length + r * 16\n",
    "    train_loader.dataset.dataset.transform = make_spec_transform(l)\n",
    "    decoder.set_output_sequence_length(l)\n",
    "    print(f\"epoch with spec length: {l}\")\n",
    "    # Train: \n",
    "    for x, _genre in (\n",
    "        progress_bar(train_loader, parent=master) if master is not None else train_loader\n",
    "    ):\n",
    "        x = x.to(device)\n",
    "        encoded = encoder(x)\n",
    "        pred = decoder(encoded)\n",
    "        loss = criterion(x, pred)\n",
    "        h.add_train_loss(loss.detach().mean())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Test:\n",
    "    with torch.no_grad():\n",
    "        for x, _genre in (\n",
    "            progress_bar(test_loader, parent=master) if master is not None else test_loader\n",
    "        ):\n",
    "            x = x.to(device)\n",
    "            encoded = encoder(x)\n",
    "            pred = decoder(encoded)\n",
    "            loss = criterion(x, pred)\n",
    "            h.add_test_loss(loss.mean())\n",
    "    \n",
    "    h.next_epoch()\n",
    "    h.write_epoch_losses()\n",
    "\n",
    "    if epoch % epoch_backup_spacing == 0:\n",
    "        test_avg, train_avg = [sum(x) / len(x) for x in [\n",
    "            h.test_losses[h.cur_epoch], h.train_losses[h.cur_epoch]\n",
    "        ]]\n",
    "        master.write(f\"Epoch {epoch}: avg test loss: {test_avg}, avg train loss: {train_avg}\")\n",
    "        torch.save(encoder.state_dict(), \"encoder_backup.pt\")\n",
    "        torch.save(decoder.state_dict(), \"decoder_backup.pt\")\n",
    "        h.write_averages(\"averages-backup.csv\")\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "torch.save(decoder.state_dict(), \"decoder.pt\")\n",
    "h.write_averages(\"averages.csv\")\n",
    "print(\"Hey it's ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_encoder = \"model-encoder.pt\"\n",
    "path_decoder = \"model-decoder.pt\"\n",
    "torch.save(encoder.state_dict(), path_encoder)\n",
    "torch.save(decoder.state_dict(), path_decoder)\n",
    "\n",
    "# reload: \n",
    "# encoder = Encoder()\n",
    "# decoder = Decoder()\n",
    "# encoder.load_state_dict(torch.load(path_encoder))\n",
    "# decoder.load_state_dict(torch.load(path_decoder))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "492dea7e935c1c914fc29e6b01e49fd41d40ab73afca215ec71ee5791b9738b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
