{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO8010: Project\n",
    "Detailed description: see [README.md](README.md)\n",
    "\n",
    "NB: references in this notebook point towards the README's references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble\n",
    "\n",
    "Importing torch and required libraries, plus miscellaneous definitions\n",
    "\n",
    "### 0.0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipynb==0.5.1\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Collecting ipywidgets==7.7.0\n",
      "  Downloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting librosa==0.9.1\n",
      "  Downloading librosa-0.9.1-py3-none-any.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib==3.5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (3.5.1)\n",
      "Collecting notebook==6.4.10\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.21.0\n",
      "  Downloading numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 20.4 MB/s eta 0:00:01    |███████████▎                    | 5.5 MB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 750.6 MB 24.2 MB/s eta 0:00:01    |██▎                             | 53.8 MB 26.6 MB/s eta 0:00:27     |██████████████████              | 420.8 MB 24.3 MB/s eta 0:00:14��█████████████▍    | 642.0 MB 22.7 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting torchaudio==0.11.0\n",
      "  Downloading torchaudio-0.11.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 34.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.12.0\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastprogress==1.0.2\n",
      "  Downloading fastprogress-1.0.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 2)) (5.1.3)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 2)) (6.9.0)\n",
      "Collecting widgetsnbextension~=3.6.0\n",
      "  Downloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 2)) (8.0.1)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (2.1.9)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (0.24.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (0.53.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (0.10.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa==0.9.1->-r requirements.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (4.29.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.1->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (22.3.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (7.1.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (3.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (6.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (4.9.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (6.4.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from notebook==6.4.10->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0->-r requirements.txt (line 7)) (4.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision==0.12.0->-r requirements.txt (line 9)) (2.26.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.0->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (2.11.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: black in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (22.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (59.5.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (3.0.26)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=5.3.4->notebook==6.4.10->-r requirements.txt (line 5)) (0.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.5.11)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook==6.4.10->-r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (18.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (3.7.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa==0.9.1->-r requirements.txt (line 3)) (0.36.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.5.1->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 9)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 9)) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 9)) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 9)) (1.26.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa==0.9.1->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa==0.9.1->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.1->-r requirements.txt (line 3)) (2.21)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook==6.4.10->-r requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (8.0.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from black->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert>=5->notebook==6.4.10->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.7.0->-r requirements.txt (line 2)) (0.8.2)\n",
      "Installing collected packages: numpy, notebook, widgetsnbextension, torch, jupyterlab-widgets, torchvision, torchaudio, librosa, ipywidgets, ipynb, fastprogress\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "  Attempting uninstall: notebook\n",
      "    Found existing installation: notebook 6.4.1\n",
      "    Uninstalling notebook-6.4.1:\n",
      "      Successfully uninstalled notebook-6.4.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.12.0a0\n",
      "    Uninstalling torchvision-0.12.0a0:\n",
      "      Successfully uninstalled torchvision-0.12.0a0\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.9.0\n",
      "    Uninstalling librosa-0.9.0:\n",
      "      Successfully uninstalled librosa-0.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed fastprogress-1.0.2 ipynb-0.5.1 ipywidgets-7.7.0 jupyterlab-widgets-1.1.0 librosa-0.9.1 notebook-6.4.10 numpy-1.21.0 torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0 widgetsnbextension-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torchdata\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as imgtransforms\n",
    "\n",
    "from typing import Tuple\n",
    "from math import ceil\n",
    "\n",
    "from torchvision import models as imgmodels\n",
    "\n",
    "from data import SpecDataset, N_MELS, HOP_LENGTHS\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Audio player & device\n",
    "\n",
    "As this features does not work on VS Code, here is a work-around stolen from [here](https://github.com/microsoft/vscode-jupyter/issues/1012#issuecomment-785410064) to have a working audio player on VSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f5be1867bd4b86a2fd1a0bc1d690c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='VS Code user ?', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vsc_check = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"VS Code user ?\",\n",
    "    disabled=False,\n",
    "    tooltip=\"Do you use VS Code to see this notebook ?\",\n",
    "    indent=False,\n",
    ")\n",
    "\n",
    "display(vsc_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import json\n",
    "\n",
    "def VSC_Audio(audio: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    Use instead of IPython.display.Audio as a workaround for VS Code.\n",
    "    `audio` is an array with shape (channels, samples) or just (samples,) for mono.\n",
    "    \"\"\"\n",
    "\n",
    "    if np.ndim(audio) == 1:\n",
    "        channels = [audio.tolist()]\n",
    "    else:\n",
    "        channels = audio.tolist()\n",
    "\n",
    "    return ipd.HTML(\"\"\"\n",
    "        <script>\n",
    "            if (!window.audioContext) {\n",
    "                window.audioContext = new AudioContext();\n",
    "                window.playAudio = function(audioChannels, sr) {\n",
    "                    const buffer = audioContext.createBuffer(audioChannels.length, audioChannels[0].length, sr);\n",
    "                    for (let [channel, data] of audioChannels.entries()) {\n",
    "                        buffer.copyToChannel(Float32Array.from(data), channel);\n",
    "                    }\n",
    "            \n",
    "                    const source = audioContext.createBufferSource();\n",
    "                    source.buffer = buffer;\n",
    "                    source.connect(audioContext.destination);\n",
    "                    source.start();\n",
    "                }\n",
    "            }\n",
    "        </script>\n",
    "        <button onclick=\"playAudio(%s, %s)\">Play</button>\n",
    "    \"\"\" % (json.dumps(channels), sr))\n",
    "\n",
    "def NonVSC_Audio(audio, sr):\n",
    "    return ipd.Audio(audio, rate=sr)\n",
    "\n",
    "audio_player = VSC_Audio if vsc_check.value else NonVSC_Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "First, define:\n",
    "1. the transformer used to pre-process the data\n",
    "1. the loader to be used to load the data for the GTZAN dataset (to start with, to be patched if not sufficient)\n",
    "\n",
    "### 1.1. Transformer\n",
    "\n",
    "**Reminder**: Mel spectrogram \"algo\"\n",
    "1. Take a window of `win_length` samples of the input at current position\n",
    "2. Compute fft with frequency spectrum is sepatated in `n_mels` evenly spaced frequencies, then apply mel scale on it (log-like), use `n_fft` samples for that computation\n",
    "3. Append the computed column to the result\n",
    "4. Move the window of `hop_length`\n",
    "5. Restart in 1.\n",
    "\n",
    "For an input of length `N`, this results in a matrix that has:\n",
    "- `N // hop_length + 1` columns\n",
    "- `n_mels` rows\n",
    "\n",
    "In our case, as input sequence are cropped at `sequence_duration` seconds, aim at:\n",
    "- Width: `ceil(sequence_duration × sampling_rate / hop_length)` columns\n",
    "- Height: `n_mels`\n",
    "\n",
    "#### Variable input length support\n",
    "\n",
    "Later on, we will require the spectrogram length (number of columns) to express like `L = 16×n + 1` so that resnet behaves well. We will directly crop the input waveform at the right length, so that we avoid to zero-pad the spectrograms what would mean an abrupt, unwanted end of the music.\n",
    "\n",
    "We get: `L = N // hop_length + 1 = 16×n+1`\n",
    "\n",
    "Hence:  `N // hop_length = 16×n`\n",
    "\n",
    "Thus:   `N = (16×hop_length)×n+r`, where `r` is an integer in the interval `[0, hop_length[`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of the sequence to randomly crop out of the original sequence\n",
    "sequence_durations = (20, 27)    # s\n",
    "\n",
    "# parameters value taken from reference [1]\n",
    "gtzan_sampling_rate = 22050 # Hz\n",
    "sampling_rate = 12000      # 12 kHz\n",
    "\n",
    "# sampling_rate is a multiple of 16 so it's fine if seconds are even for \n",
    "# the spectrograms to shape like 16×n+1 for the resnet nice behaviour\n",
    "min_spec_length_seconds = 20\n",
    "max_spec_length_seconds = 28\n",
    "min_spec_length, max_spec_length = [int((x * sampling_rate) / HOP_LENGTHS[1] + 1) for x in [\n",
    "    min_spec_length_seconds, max_spec_length_seconds\n",
    "]]\n",
    "\n",
    "class OurRandomCrop(object):\n",
    "    \"\"\"\n",
    "    Cannot work because of concurrency, and have to stack tensors of same dimensions\n",
    "    in a same minibatch...\n",
    "    \"\"\"\n",
    "    def __init__(self, height: int, bounds: Tuple[int, int], step: int = 1):\n",
    "        self.height = height\n",
    "        self.base = bounds[0]\n",
    "        self.num = int(ceil((bounds[1] - bounds[0]) / step))\n",
    "        self.step = step\n",
    "        self.batch_size = 5\n",
    "        self.counter = 0\n",
    "    \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.counter == 0:\n",
    "            self.counter = self.batch_size - 1\n",
    "            self.r = int(torch.randint(self.num, (1, 1)))\n",
    "        self.counter = (self.counter + 1) % 5\n",
    "        l = self.base + self.r * self.step\n",
    "        return imgtransforms.RandomCrop((self.height, l))(x)\n",
    "    \n",
    "\n",
    "def make_spec_transform(spec_length: int):\n",
    "    return imgtransforms.Compose([\n",
    "        imgtransforms.RandomCrop((N_MELS, spec_length)),\n",
    "        imgtransforms.RandomApply([\n",
    "            imgtransforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.4, hue=0.1),\n",
    "            imgtransforms.GaussianBlur(3, sigma=(0.1, 1)),\n",
    "            imgtransforms.RandomErasing(p=0.4)\n",
    "        ], p=0.7)\n",
    "    ])\n",
    "\n",
    "transform = make_spec_transform(min_spec_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset and data loader\n",
    "\n",
    "We split the original dataset (1000 sequences long) into test and train set:\n",
    "- Training set will be 70% of the dataset\n",
    "- Test set will contain the remaining sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_workers = 1\n",
    "\n",
    "dataset = SpecDataset(\n",
    "    \"data/gtzan\",\n",
    "    sampling_rate=sampling_rate,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "trainset_length = int(0.7 * dataset_length)\n",
    "testset_length = dataset_length - trainset_length\n",
    "trainset, testset = torchdata.random_split(dataset, [trainset_length, testset_length])\n",
    "\n",
    "data_loader = torchdata.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "train_loader = torchdata.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torchdata.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: an element `spec` of the dataset has the dimensions: (batch, channel, frequency, time), where\n",
    "- batch correspond to the id of the sample in the batch\n",
    "- channel is artificial construction to use resnet models, spans across the different spectrograms\n",
    "- frequency spans across the frequency range of the window\n",
    "- time indicate the time of the window\n",
    "\n",
    "### 1.3. Test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec shape: torch.Size([5, 3, 128, 801])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7febc321a8b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec, genre = next(iter(data_loader))\n",
    "print(f\"spec shape: {spec.shape}\")\n",
    "plt.imshow(spec[0][0], vmin=0, vmax=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the encoder and the decoder\n",
    "\n",
    "Define as `nn.Module`s:\n",
    "- the encoder\n",
    "   - Input: waveform\n",
    "   - Output: latent representation\n",
    "- the decoder\n",
    "   - Input: latent representation\n",
    "   - Ouptput: waveform\n",
    "- an encoder-decoder that chains them both for convenience\n",
    "\n",
    "### Shapes\n",
    "| Stage                  | Shape            | Meaning                      |\n",
    "| ---------------------- | ---------------- | ---------------------------- |\n",
    "| Input `spec`           | [5, 3, 128, 801] | (batch, channel, freq, time) |\n",
    "| Output of `ResNet34Cut`| [5, 256, 8, 51]  | (batch, stack, freq2, time2) |\n",
    "| Intput of LSTM         | [5, 2048, 51]    | (batch, freq3, time3)        |\n",
    "| Output of LSTM         | [5, 2048]        | (batch, component)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resnet utilities\n",
    "\n",
    "Cut the resnet34 architecture to keep more detailed information (and not loose temporal dimension):\n",
    "\n",
    "NB: the `forward` implementation is almost copy-pasted from the torchvision sources, from [here](https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet34).\n",
    "\n",
    "Also, define an inverse module for that architecture to be used in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34Cut(imgmodels.ResNet):\n",
    "    def __init__(self, pretrained=True):\n",
    "        resnet = imgmodels.resnet34(pretrained=pretrained)\n",
    "        for k, v in resnet.__dict__.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "        del self.layer4\n",
    "        del self.avgpool\n",
    "        del self.fc\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.size = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x.size = [batch(B), channels(256), height(8), length(f(L)=N+1)]\n",
    "        # f(L) = ???, at least f(16×n+1) = n+1   for int n\n",
    "        #         ↳ Because of mess of conv & pooling layers in resnet\n",
    "\n",
    "        return x\n",
    "\n",
    "class InverseResNet34Cut(nn.Module):\n",
    "    channels = [256, 128, 64, 64, 3]\n",
    "    kernel_sizes = [(3, 3), (3, 3), (3, 3), (3, 3)]\n",
    "    mid_kernel_size = (3, 3)\n",
    "    num_mid_layers = [2, 1, 1, 0]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = self._make_layer(\n",
    "            in_channels = self.channels[0],\n",
    "            out_channels = self.channels[1],\n",
    "            kernel_size = self.kernel_sizes[0],\n",
    "            output_padding = (1, 0),\n",
    "            num_mid_layers = self.num_mid_layers[0],\n",
    "            mid_kernel_size = self.mid_kernel_size,\n",
    "        )\n",
    "        \n",
    "        self.layer2 = self._make_layer(\n",
    "            in_channels = self.channels[1], \n",
    "            out_channels = self.channels[2],\n",
    "            kernel_size = self.kernel_sizes[1],\n",
    "            num_mid_layers = self.num_mid_layers[1],\n",
    "            mid_kernel_size = self.mid_kernel_size\n",
    "        )\n",
    "        \n",
    "        self.layer3 = self._make_layer(\n",
    "            in_channels = self.channels[2], \n",
    "            out_channels = self.channels[3],\n",
    "            kernel_size = self.kernel_sizes[2],\n",
    "            num_mid_layers =self.num_mid_layers[2],\n",
    "            mid_kernel_size = self.mid_kernel_size\n",
    "        )\n",
    "\n",
    "        self.layer4 = self._make_layer(\n",
    "            in_channels = self.channels[3],\n",
    "            out_channels = self.channels[4],\n",
    "            kernel_size = self.kernel_sizes[3],\n",
    "            num_mid_layers = self.num_mid_layers[3],\n",
    "            mid_kernel_size = self.mid_kernel_size,\n",
    "        )\n",
    "    \n",
    "    def _make_transition_layer(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, int],\n",
    "        padding: Tuple[int, int] = (1, 1),\n",
    "        output_padding: Tuple[int, int] = (1, 0)\n",
    "    ) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=(2, 2),\n",
    "                padding=padding,\n",
    "                output_padding=output_padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.01),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def _make_mid_layer(self, channels: int, kernel_size: int) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=channels,\n",
    "                out_channels=channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(channels, momentum=0.01),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def _make_layer(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Tuple[int, int],\n",
    "        num_mid_layers: int,\n",
    "        mid_kernel_size: Tuple[int, int],\n",
    "        padding: Tuple[int, int] = (1, 1),\n",
    "        output_padding: Tuple[int, int] = (1, 0),\n",
    "    ) -> nn.Module:\n",
    "        l = []\n",
    "        l.append(self._make_transition_layer(\n",
    "            in_channels, out_channels, kernel_size, padding, output_padding\n",
    "        ))\n",
    "        for _ in range(num_mid_layers):\n",
    "            l.append(self._make_mid_layer(out_channels, mid_kernel_size))\n",
    "        \n",
    "        return nn.Sequential(*l)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(f(L))]\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "x.shape = torch.Size([1, 3, 128, 17])\n",
      "y.shape = torch.Size([1, 256, 8, 2])\n",
      "z.shape = torch.Size([1, 3, 128, 17])\n"
     ]
    }
   ],
   "source": [
    "# test shapes are coherent\n",
    "resnet = ResNet34Cut()\n",
    "inverse = InverseResNet34Cut()\n",
    "\n",
    "resnet.to(device)\n",
    "inverse.to(device)\n",
    "x = torch.randn([1, 3, 128, 17]).to(device)\n",
    "print(type(x.to(device)))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "y = resnet(x)\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "z = inverse(y)\n",
    "print(f\"z.shape = {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder and decoder\n",
    "\n",
    "Remark: the LSTM input size is constant, as it is derived from the dimension of the output of the cut resnet model, that is the product between the number of channels (256, fixed by ResNet34Cut) and the height of the output, that evaluates to `post_resnet_height = n_mels / 16 = 8` as long as `n_mels` remains 128.\n",
    "\n",
    "**Recall**\n",
    "In pytorch, a mono-directional LSTM set with `batch_first=True`, having `num_lstm_layers` LSTMs stacked on each other, takes as inputs:\n",
    "- the input sequence `x`, of shape `[batch, sequence, H_out]`\n",
    "- the initial hidden state `h_0`, of shape `[num_lstm_layers, batch, H_out]`\n",
    "- the initial cell state `c_0`, of shape  `[num_lstm_layers, batch, H_out]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_resnet_height: int = int(N_MELS / 16)\n",
    "lstm_input_size: int = 256 * post_resnet_height\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 2048,\n",
    "        num_lstm_layers: int = 1,\n",
    "        output_size: int = 2048\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.resnet = ResNet34Cut(pretrained=True)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.h0 = nn.parameter.Parameter(torch.zeros(hidden_size))\n",
    "        self.c0 = nn.parameter.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.init_state = None\n",
    "    \n",
    "    def set_init_state(self, s: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        self.init_state = s\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.resnet(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        length = x.shape[-1]\n",
    "        x = x.view(batch_size, -1, length).swapaxes(1, 2)\n",
    "\n",
    "        # x.shape = [batch(B), length(N+1), height(lstm_input_size=2048)]\n",
    "        if self.init_state is not None:\n",
    "            output, (hn, cn) = self.lstm(x, self.init_state)\n",
    "            self.init_state = None\n",
    "        else:\n",
    "            h0 = self.h0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            c0 = self.c0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # hn.shape = [num_lstm_layers, batch(B), hidden]\n",
    "        x = self.fc(hn.squeeze(0))\n",
    "        \n",
    "        # x.shape = [batch(B), output_size]\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(output_size, hidden_size)\n",
    "\n",
    "        self.const_x = nn.parameter.Parameter(torch.zeros(lstm_input_size))\n",
    "        self.c0 = nn.parameter.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.reverse_resnet = InverseResNet34Cut()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def set_output_sequence_length(self, spec_length: int):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output spectrograms of length `length`, \n",
    "        parameterizing the LSTM input.\n",
    "        \"\"\"\n",
    "        length = int((spec_length-1) // 16 + 1)\n",
    "        self.sequence_length = length\n",
    "    \n",
    "    def set_output_sequence_length_seconds(self, seconds: float):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output approximately `seconds` seconds spectrograms.\n",
    "        Relies on the MelSpectrogram class used, obviously.\n",
    "\n",
    "        Dumby interpolation from 801 spectrogram length corresponding to ±20s,\n",
    "        hence 40 length is 1s.\n",
    "        \"\"\"\n",
    "        self.set_output_sequence_length(int(40 * seconds))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), output_size]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # x.shape = [batch(B), hidden_size]\n",
    "        lstm_input = self.const_x.repeat(batch_size, self.sequence_length, 1).to(device)\n",
    "        c0 = self.c0.repeat(batch_size, 1).unsqueeze(0)\n",
    "        x, (hn, cn) = self.lstm(lstm_input, (x.unsqueeze(0), c0))\n",
    "        \n",
    "        # x.shape = [batch(B), length(N+1), hidden_size]\n",
    "        x = x.swapaxes(1, 2).view(batch_size, 256, post_resnet_height, self.sequence_length)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        x = self.reverse_resnet(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(16×N+1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_input_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v.shape = torch.Size([5, 2048])\n",
      "y.shape = torch.Size([5, 3, 128, 817])\n"
     ]
    }
   ],
   "source": [
    "# testing...\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "x = torch.randn([5, 3, 128, 817])\n",
    "decoder.set_output_sequence_length(817)\n",
    "v = encoder(x.to(device))\n",
    "print(f\"v.shape = {v.shape}\")\n",
    "y = decoder(v)\n",
    "print(f\"y.shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training the model\n",
    "\n",
    "### 3.0 Utility for storing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossesHandler(object):\n",
    "    def __init__(self, test_name: str, train_name: str):\n",
    "        self.ftest = open(test_name, \"w\")\n",
    "        self.ftrain = open(train_name, \"w\")\n",
    "        # self.losses[e][i] contains the loss at epoch e for elm i\n",
    "        self.train_losses = [[]]\n",
    "        self.test_losses = [[]]\n",
    "        self.spec_lengths = []\n",
    "        self.cur_epoch = 0\n",
    "    \n",
    "    def add_test_loss(self, test_loss: float):\n",
    "        self.test_losses[self.cur_epoch].append(test_loss)\n",
    "    \n",
    "    def add_train_loss(self, train_loss: float):\n",
    "        self.train_losses[self.cur_epoch].append(train_loss)\n",
    "    \n",
    "    def next_epoch(self, past_spec_length):\n",
    "        self.train_losses.append([])\n",
    "        self.test_losses.append([])\n",
    "        self.spec_lengths.append(past_spec_length)\n",
    "        self.cur_epoch += 1\n",
    "    \n",
    "    def write_epoch_losses(self):\n",
    "        self.ftrain.write(\", \".join([str(l) for l in self.train_losses[self.cur_epoch]]) + \"\\n\")\n",
    "        self.ftest.write(\", \".join([str(l) for l in self.test_losses[self.cur_epoch]]) + \"\\n\")\n",
    "    \n",
    "    def write_averages(self, filename: str):\n",
    "        f = open(filename, \"w\")\n",
    "        f.write(\"Epoch number,Epoch spec length,Average test loss,Average train loss\\n\")\n",
    "        for i, (test, train, spec_length) in enumerate(zip(self.test_losses, self.train_losses, self.spec_lengths)):\n",
    "            avg_test, avg_train = [sum(x) / len(x) for x in [test, train]]\n",
    "            f.write(f\"{i},{spec_length},{avg_test},{avg_train}\\n\")\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.ftrain.close()\n",
    "        self.ftest.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "0 -> 1.0576294506847717e-08\n",
      "1 -> 2.400729925966516e-08\n",
      "2 -> 0.0001158615414169617\n",
      "3 -> 4.586567229125649e-05\n",
      "4 -> 2.226423930551391e-05\n",
      "5 -> 0.00010987134737661108\n",
      "6 -> 2.759023664111737e-05\n",
      "7 -> 2.9057462597847916e-05\n",
      "8 -> 0.0001239188277395442\n",
      "9 -> 3.7931073165964335e-05\n",
      "10 -> 1.8917853594757617e-05\n",
      "11 -> 0.00013937870971858501\n",
      "12 -> 3.264846600359306e-05\n",
      "13 -> 3.0022618375369348e-05\n",
      "14 -> 0.0001448435359634459\n",
      "15 -> 2.9331646146601997e-05\n",
      "16 -> 1.6011050320230424e-05\n",
      "17 -> 0.0001519569632364437\n",
      "18 -> 3.64935549441725e-05\n",
      "19 -> 2.6362738935858943e-05\n",
      "20 -> 0.00014455927885137498\n",
      "21 -> 2.333555676159449e-05\n",
      "22 -> 1.2061384040862322e-05\n",
      "23 -> 0.0002317788457730785\n",
      "24 -> 4.801022805622779e-05\n",
      "25 -> 3.09472088702023e-05\n",
      "26 -> 0.00020744190260302275\n",
      "27 -> 3.941405157092959e-05\n",
      "28 -> 2.5163528334815055e-05\n",
      "29 -> 0.00012052381498506293\n",
      "30 -> 4.3948894017376006e-05\n",
      "31 -> 2.5163528334815055e-05\n",
      "32 -> 0.00016452018462587148\n",
      "33 -> 3.580368138500489e-05\n",
      "34 -> 2.4307440980919637e-05\n",
      "35 -> 0.00018847614410333335\n",
      "36 -> 5.584550672210753e-05\n",
      "37 -> 2.5285558876930736e-05\n",
      "38 -> 0.00020575628150254488\n",
      "39 -> 6.644630775554106e-05\n",
      "40 -> 2.5454966817051172e-05\n",
      "41 -> 0.0002652794064488262\n",
      "42 -> 6.409212073776871e-05\n",
      "43 -> 2.5925235604518093e-05\n",
      "44 -> 0.00028168808785267174\n",
      "45 -> 6.91921086399816e-05\n",
      "46 -> 2.9932996767456643e-05\n",
      "47 -> 0.00028642479446716607\n",
      "48 -> 8.888824231689796e-05\n",
      "49 -> 2.1950552763883024e-05\n",
      "50 -> 0.0004758502182085067\n",
      "51 -> 0.00010473312431713566\n",
      "52 -> 3.949239544454031e-05\n",
      "53 -> 0.000633877469226718\n",
      "54 -> 0.0001414841099176556\n",
      "55 -> 4.735220500151627e-05\n",
      "56 -> 0.0003940951719414443\n",
      "57 -> 0.0001775034179445356\n",
      "58 -> 4.735220500151627e-05\n",
      "59 -> 0.0003823022707365453\n",
      "60 -> 0.0001002403732854873\n",
      "61 -> 3.180565909133293e-05\n",
      "62 -> 0.0005847265128977597\n",
      "63 -> 0.00017642932652961463\n",
      "64 -> 4.997876021661796e-05\n",
      "65 -> 0.0005125520983710885\n",
      "66 -> 0.00013362355821300298\n",
      "67 -> 3.7339083064580336e-05\n",
      "68 -> 0.000846449809614569\n",
      "69 -> 0.00018593986169435084\n",
      "70 -> 4.994863775209524e-05\n",
      "71 -> 0.000538561143912375\n",
      "72 -> 0.0001528669090475887\n",
      "73 -> 4.6200279030017555e-05\n",
      "74 -> 0.0009327615262009203\n",
      "75 -> 0.0002532961079850793\n",
      "76 -> 5.121428330312483e-05\n",
      "77 -> 0.0007617692463099957\n",
      "78 -> 0.00018246733816340566\n",
      "79 -> 4.606533912010491e-05\n",
      "80 -> 0.0011450276942923665\n",
      "81 -> 0.0002700171316973865\n",
      "82 -> 5.205470370128751e-05\n",
      "83 -> 0.0004413695714902133\n",
      "84 -> 0.00013387459330260754\n",
      "85 -> 2.8853522962890565e-05\n",
      "86 -> 0.0008927287417463958\n",
      "87 -> 0.00037059379974380136\n",
      "88 -> 5.832219903822988e-05\n",
      "89 -> 0.010623565874993801\n",
      "90 -> 0.002523136790841818\n",
      "91 -> 7.591665053041652e-05\n",
      "92 -> 7.591665053041652e-05\n",
      "93 -> 0.14833112061023712\n",
      "94 -> 0.004452809225767851\n",
      "95 -> 1.601328730583191\n",
      "96 -> 2.1549906730651855\n",
      "97 -> 1.3900657892227173\n",
      "98 -> 0.004647002089768648\n",
      "99 -> 0.5802665948867798\n",
      "100 -> 47.22958755493164\n",
      "101 -> 1.8686367273330688\n",
      "102 -> 1.8686367273330688\n",
      "103 -> 379.0218811035156\n",
      "104 -> 1.4057158296054695e-05\n",
      "105 -> 16.934715270996094\n",
      "106 -> 10.100029945373535\n",
      "107 -> 427.75616455078125\n",
      "108 -> 1.6920945199672133e-05\n",
      "109 -> 28.546974182128906\n",
      "110 -> 26.24100685119629\n",
      "111 -> 712.2324829101562\n",
      "112 -> 1.3126224985171575e-05\n",
      "113 -> 134.4671630859375\n",
      "114 -> 80.67342376708984\n",
      "115 -> 1518.6058349609375\n",
      "116 -> 7.545169501099735e-05\n",
      "117 -> 415.0641174316406\n",
      "118 -> 163.1031494140625\n",
      "119 -> 3736.983642578125\n",
      "120 -> 8.048238669289276e-05\n",
      "121 -> 2403.794921875\n",
      "122 -> 1054.176513671875\n",
      "123 -> 12302.0078125\n",
      "124 -> 0.0009161317138932645\n",
      "125 -> 4787.96435546875\n",
      "126 -> 7138.42041015625\n",
      "127 -> 28530.091796875\n",
      "128 -> 0.0008916704682633281\n",
      "129 -> 43814.45703125\n",
      "130 -> 16807.8359375\n",
      "131 -> 132112.375\n",
      "132 -> 0.0038703219033777714\n",
      "133 -> 85928.7265625\n",
      "134 -> 128251.1875\n"
     ]
    }
   ],
   "source": [
    "print(len(params))\n",
    "#for i, p in enumerate(params):\n",
    "    #n = torch.norm(p.grad.detach(), 2)\n",
    "    #print(f\"{i} -> {n}\")\n",
    "for i, p in enumerate(params):\n",
    "    print(f\"{i} -> {torch.norm(p.grad.detach(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='22' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.33% [22/300 19:22<4:04:48]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1: avg test loss: 645098.5625, avg train loss: 955101.6875, Spec length: 897<p>Epoch 2: avg test loss: 1226202.125, avg train loss: 1070240.875, Spec length: 913<p>Epoch 3: avg test loss: 1156954.375, avg train loss: 1151413.0, Spec length: 993<p>Epoch 4: avg test loss: 1112220.125, avg train loss: 921992.1875, Spec length: 1041<p>Epoch 5: avg test loss: 840140.0, avg train loss: 789425.3125, Spec length: 961<p>Epoch 6: avg test loss: 1018719.5, avg train loss: 972616.375, Spec length: 817<p>Epoch 7: avg test loss: 1257447.25, avg train loss: 1182326.875, Spec length: 961<p>Epoch 8: avg test loss: 866557.0, avg train loss: 1154099.375, Spec length: 913<p>Epoch 9: avg test loss: 659866.75, avg train loss: 747202.5, Spec length: 961<p>Epoch 10: avg test loss: 1282420.375, avg train loss: 920913.0625, Spec length: 993<p>Epoch 11: avg test loss: 1004019.5, avg train loss: 1142498.25, Spec length: 977<p>Epoch 12: avg test loss: 596693.1875, avg train loss: 816215.0, Spec length: 817<p>Epoch 13: avg test loss: 1125727.25, avg train loss: 1001460.375, Spec length: 1041<p>Epoch 14: avg test loss: 957049.0, avg train loss: 1148411.0, Spec length: 913<p>Epoch 15: avg test loss: 1091797.875, avg train loss: 936288.625, Spec length: 961<p>Epoch 16: avg test loss: 1141175.75, avg train loss: 899397.4375, Spec length: 1057<p>Epoch 17: avg test loss: 1039933.4375, avg train loss: 846035.8125, Spec length: 929<p>Epoch 18: avg test loss: 1051071.0, avg train loss: 1098773.875, Spec length: 881<p>Epoch 19: avg test loss: 1095749.375, avg train loss: 1103707.75, Spec length: 929<p>Epoch 20: avg test loss: 1197121.75, avg train loss: 989191.75, Spec length: 1041<p>Epoch 21: avg test loss: 887618.875, avg train loss: 887306.5, Spec length: 929<p>Epoch 22: avg test loss: 926720.875, avg train loss: 885219.625, Spec length: 961<p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='21' class='' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.00% [21/140 00:06<00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#grad_norm = torch.norm(\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#    torch.stack([torch.norm(p.grad.detach(), 2) for p in decoder.parameters()]), 2)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Test:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "epoch_backup_spacing = 1\n",
    "h = LossesHandler(\"test_losses.csv\", \"train_losses.csv\")\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# spec lengths chosen randomly at each epochs\n",
    "num_choices = int((max_spec_length - min_spec_length) / 16)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "master = master_bar(range(1, num_epochs+1))\n",
    "\n",
    "for epoch in master:\n",
    "    r = int(torch.randint(num_choices, size=(1, 1)))\n",
    "    l = min_spec_length + r * 16\n",
    "    train_loader.dataset.dataset.transform = make_spec_transform(l)\n",
    "    decoder.set_output_sequence_length(l)\n",
    "    # Train: \n",
    "    for x, _genre in (\n",
    "        progress_bar(train_loader, parent=master) if master is not None else train_loader\n",
    "    ):\n",
    "        x = x.to(device)\n",
    "        encoded = encoder(x)\n",
    "        pred = decoder(encoded)\n",
    "        loss = criterion(x, pred) \n",
    "        h.add_train_loss(loss.detach().mean())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #grad_norm = torch.norm(\n",
    "        #    torch.stack([torch.norm(p.grad.detach(), 2) for p in decoder.parameters()]), 2)\n",
    "\n",
    "        # print(f\"Grad norm: {grad_norm}\")\n",
    "\n",
    "    \n",
    "    # Test:\n",
    "    with torch.no_grad():\n",
    "        for x, _genre in (\n",
    "            progress_bar(test_loader, parent=master) if master is not None else test_loader\n",
    "        ):\n",
    "            x = x.to(device)\n",
    "            encoded = encoder(x)\n",
    "            pred = decoder(encoded)\n",
    "            loss = criterion(x, pred)\n",
    "            h.add_test_loss(loss.mean())\n",
    "    \n",
    "    h.write_epoch_losses()\n",
    "    h.next_epoch(l)\n",
    "\n",
    "    if epoch % epoch_backup_spacing == 0:\n",
    "        test_avg, train_avg = [sum(x) / len(x) for x in [\n",
    "            h.test_losses[h.cur_epoch-1], h.train_losses[h.cur_epoch-1]\n",
    "        ]]\n",
    "        master.write(f\"Epoch {epoch}: avg test loss: {test_avg}, avg train loss: {train_avg}, \" +\n",
    "                     f\"Spec length: {l}\")\n",
    "        torch.save(encoder.state_dict(), \"encoder_backup.pt\")\n",
    "        torch.save(decoder.state_dict(), \"decoder_backup.pt\")\n",
    "        h.write_averages(\"averages-backup.csv\")\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "torch.save(decoder.state_dict(), \"decoder.pt\")\n",
    "h.write_averages(\"averages.csv\")\n",
    "print(\"Hey it's ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_encoder = \"model-encoder.pt\"\n",
    "# path_decoder = \"model-decoder.pt\"\n",
    "# torch.save(encoder.state_dict(), path_encoder)\n",
    "# torch.save(decoder.state_dict(), path_decoder)\n",
    "\n",
    "# reload: \n",
    "# encoder = Encoder()\n",
    "# decoder = Decoder()\n",
    "# encoder.load_state_dict(torch.load(path_encoder))\n",
    "# decoder.load_state_dict(torch.load(path_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avgs = pd.read_csv(\"averages.csv\")\n",
    "plt.figure()\n",
    "plt.plot(avgs[\"Epoch number\"], avgs[\"Average test loss\"])\n",
    "plt.plot(avgs[\"Epoch number\"], avgs[\"Average train loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(7)\n",
    "x = x.expand(4, 2, 1)\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "492dea7e935c1c914fc29e6b01e49fd41d40ab73afca215ec71ee5791b9738b1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
