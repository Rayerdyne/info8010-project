{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO8010: Project\n",
    "Detailed description: see [README.md](README.md)\n",
    "\n",
    "NB: references in this notebook point towards the README's references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble\n",
    "\n",
    "Importing torch and required libraries, plus miscellaneous definitions\n",
    "\n",
    "### 0.0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'pandas=1.4.2' (from line 11 of requirements.txt)\n",
      "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Imports & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torchdata\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as imgtransforms\n",
    "\n",
    "from typing import Tuple\n",
    "from math import ceil\n",
    "\n",
    "from torchvision import models as imgmodels\n",
    "\n",
    "from data import SpecDataset, N_MELS, HOP_LENGTHS\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Audio player\n",
    "\n",
    "As this features does not work on VS Code, here is a work-around stolen from [here](https://github.com/microsoft/vscode-jupyter/issues/1012#issuecomment-785410064) to have a working audio player on VSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a36ad2c9ec482a999f449f9c3e5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='VS Code user ?', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "vsc_check = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"VS Code user ?\",\n",
    "    disabled=False,\n",
    "    tooltip=\"Do you use VS Code to see this notebook ?\",\n",
    "    indent=False,\n",
    ")\n",
    "\n",
    "display(vsc_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import json\n",
    "\n",
    "def VSC_Audio(audio: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    Use instead of IPython.display.Audio as a workaround for VS Code.\n",
    "    `audio` is an array with shape (channels, samples) or just (samples,) for mono.\n",
    "    \"\"\"\n",
    "\n",
    "    if np.ndim(audio) == 1:\n",
    "        channels = [audio.tolist()]\n",
    "    else:\n",
    "        channels = audio.tolist()\n",
    "\n",
    "    return ipd.HTML(\"\"\"\n",
    "        <script>\n",
    "            if (!window.audioContext) {\n",
    "                window.audioContext = new AudioContext();\n",
    "                window.playAudio = function(audioChannels, sr) {\n",
    "                    const buffer = audioContext.createBuffer(audioChannels.length, audioChannels[0].length, sr);\n",
    "                    for (let [channel, data] of audioChannels.entries()) {\n",
    "                        buffer.copyToChannel(Float32Array.from(data), channel);\n",
    "                    }\n",
    "            \n",
    "                    const source = audioContext.createBufferSource();\n",
    "                    source.buffer = buffer;\n",
    "                    source.connect(audioContext.destination);\n",
    "                    source.start();\n",
    "                }\n",
    "            }\n",
    "        </script>\n",
    "        <button onclick=\"playAudio(%s, %s)\">Play</button>\n",
    "    \"\"\" % (json.dumps(channels), sr))\n",
    "\n",
    "def NonVSC_Audio(audio, sr):\n",
    "    return ipd.Audio(audio, rate=sr)\n",
    "\n",
    "audio_player = VSC_Audio if vsc_check.value else NonVSC_Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "First, define:\n",
    "1. the transformer used to pre-process the data\n",
    "1. the loader to be used to load the data for the GTZAN dataset (to start with, to be patched if not sufficient)\n",
    "\n",
    "### 1.1. Transformer\n",
    "\n",
    "**Reminder**: Mel spectrogram \"algo\"\n",
    "1. Take a window of `win_length` samples of the input at current position\n",
    "2. Compute fft with frequency spectrum is sepatated in `n_mels` evenly spaced frequencies, then apply mel scale on it (log-like), use `n_fft` samples for that computation\n",
    "3. Append the computed column to the result\n",
    "4. Move the window of `hop_length`\n",
    "5. Restart in 1.\n",
    "\n",
    "For an input of length `N`, this results in a matrix that has:\n",
    "- `N // hop_length + 1` columns\n",
    "- `n_mels` rows\n",
    "\n",
    "In our case, as input sequence are cropped at `sequence_duration` seconds, aim at:\n",
    "- Width: `ceil(sequence_duration × sampling_rate / hop_length)` columns\n",
    "- Height: `n_mels`\n",
    "\n",
    "#### Variable input length support\n",
    "\n",
    "Later on, we will require the spectrogram length (number of columns) to express like `L = 16×n + 1` so that resnet behaves well. We will directly crop the input waveform at the right length, so that we avoid to zero-pad the spectrograms what would mean an abrupt, unwanted end of the music.\n",
    "\n",
    "We get: `L = N // hop_length + 1 = 16×n+1`\n",
    "\n",
    "Hence:  `N // hop_length = 16×n`\n",
    "\n",
    "Thus:   `N = (16×hop_length)×n+r`, where `r` is an integer in the interval `[0, hop_length[`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of the sequence to randomly crop out of the original sequence\n",
    "sequence_durations = (20, 27)    # s\n",
    "\n",
    "# parameters value taken from reference [1]\n",
    "gtzan_sampling_rate = 22050 # Hz\n",
    "sampling_rate = 12000      # 12 kHz\n",
    "\n",
    "# sampling_rate is a multiple of 16 so it's fine if seconds are even for \n",
    "# the spectrograms to shape like 16×n+1 for the resnet nice behaviour\n",
    "min_spec_length_seconds = 20\n",
    "max_spec_length_seconds = 28\n",
    "min_spec_length, max_spec_length = [int((x * sampling_rate) / HOP_LENGTHS[1] + 1) for x in [\n",
    "    min_spec_length_seconds, max_spec_length_seconds\n",
    "]]\n",
    "\n",
    "class OurRandomCrop(object):\n",
    "    \"\"\"\n",
    "    Cannot work because of concurrency, and have to stack tensors of same dimensions\n",
    "    in a same minibatch...\n",
    "    \"\"\"\n",
    "    def __init__(self, height: int, bounds: Tuple[int, int], step: int = 1):\n",
    "        self.height = height\n",
    "        self.base = bounds[0]\n",
    "        self.num = int(ceil((bounds[1] - bounds[0]) / step))\n",
    "        self.step = step\n",
    "        self.batch_size = 5\n",
    "        self.counter = 0\n",
    "    \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.counter == 0:\n",
    "            self.counter = self.batch_size - 1\n",
    "            self.r = int(torch.randint(self.num, (1, 1)))\n",
    "        self.counter = (self.counter + 1) % 5\n",
    "        l = self.base + self.r * self.step\n",
    "        return imgtransforms.RandomCrop((self.height, l))(x)\n",
    "    \n",
    "\n",
    "def make_spec_transform(spec_length: int):\n",
    "    return imgtransforms.Compose([\n",
    "        imgtransforms.RandomCrop((N_MELS, spec_length)),\n",
    "        imgtransforms.RandomApply([\n",
    "            imgtransforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.4, hue=0.1),\n",
    "            imgtransforms.GaussianBlur(3, sigma=(0.1, 1)),\n",
    "            imgtransforms.RandomErasing(p=0.4)\n",
    "        ], p=0.7)\n",
    "    ])\n",
    "\n",
    "transform = make_spec_transform(min_spec_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset and data loader\n",
    "\n",
    "We split the original dataset (1000 sequences long) into test and train set:\n",
    "- Training set will be 70% of the dataset\n",
    "- Test set will contain the remaining sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_workers = 1\n",
    "\n",
    "dataset = SpecDataset(\n",
    "    \"data/gtzan\",\n",
    "    sampling_rate=sampling_rate,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "dataset_length = len(dataset)\n",
    "trainset_length = int(0.7 * dataset_length)\n",
    "testset_length = dataset_length - trainset_length\n",
    "trainset, testset = torchdata.random_split(dataset, [trainset_length, testset_length])\n",
    "\n",
    "data_loader = torchdata.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "train_loader = torchdata.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torchdata.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: an element `spec` of the dataset has the dimensions: (batch, channel, frequency, time), where\n",
    "- batch correspond to the id of the sample in the batch\n",
    "- channel is artificial construction to use resnet models, spans across the different spectrograms\n",
    "- frequency spans across the frequency range of the window\n",
    "- time indicate the time of the window\n",
    "\n",
    "### 1.3. Test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec shape: torch.Size([5, 3, 128, 801])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8ae5339580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec, genre = next(iter(data_loader))\n",
    "print(f\"spec shape: {spec.shape}\")\n",
    "plt.imshow(spec[0][0], vmin=0, vmax=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the encoder and the decoder\n",
    "\n",
    "Define as `nn.Module`s:\n",
    "- the encoder\n",
    "   - Input: waveform\n",
    "   - Output: latent representation\n",
    "- the decoder\n",
    "   - Input: latent representation\n",
    "   - Ouptput: waveform\n",
    "- an encoder-decoder that chains them both for convenience\n",
    "\n",
    "### Shapes\n",
    "| Stage                  | Shape            | Meaning                      |\n",
    "| ---------------------- | ---------------- | ---------------------------- |\n",
    "| Input `spec`           | [5, 3, 128, 801] | (batch, channel, freq, time) |\n",
    "| Output of `ResNet34Cut`| [5, 256, 8, 51]  | (batch, stack, freq2, time2) |\n",
    "| Intput of LSTM         | [5, 2048, 51]    | (batch, freq3, time3)        |\n",
    "| Output of LSTM         | [5, 2048]        | (batch, component)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resnet utilities\n",
    "\n",
    "Cut the resnet34 architecture to keep more detailed information (and not loose temporal dimension):\n",
    "\n",
    "NB: the `forward` implementation is almost copy-pasted from the torchvision sources, from [here](https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet34).\n",
    "\n",
    "Also, define an inverse module for that architecture to be used in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = [3, 10, 20]\n",
    "kernel_sizes = [(5, 3), (5, 3)]\n",
    "strides = [(2, 2), (2, 2)]\n",
    "# strides = [(1, 1), (1, 1)]\n",
    "\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels[0], num_channels[1], kernel_sizes[0], strides[0]),\n",
    "            nn.MaxPool2d(kernel_sizes[0], (1, 1), padding=(2, 1)),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels[1], num_channels[2], kernel_sizes[1], strides[1]),\n",
    "            nn.MaxPool2d(kernel_sizes[1], (1, 1), padding=(2, 1)),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.size = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        # x = self.layer1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        # x.size = [batch(B), channels(20), height(x), length(f(L)=N+1)]\n",
    "\n",
    "        return x\n",
    "\n",
    "class InverseCNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_channels[2], num_channels[1], kernel_sizes[1], strides[1], output_padding=(1, 1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_channels[1], num_channels[0], kernel_sizes[0], strides[0], output_padding=(1, 0)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(f(L))]\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "x.shape = torch.Size([1, 3, 128, 1085])\n",
      "inner: x.shape: torch.Size([1, 10, 62, 542])\n",
      "y.shape = torch.Size([1, 20, 29, 270])\n",
      "inner: torch.Size([1, 10, 62, 542])\n",
      "z.shape = torch.Size([1, 3, 128, 1085])\n"
     ]
    }
   ],
   "source": [
    "# test shapes are coherent\n",
    "cnn = CNNLayer()\n",
    "invcnn = InverseCNNLayer()\n",
    "\n",
    "cnn.to(device)\n",
    "invcnn.to(device)\n",
    "x = torch.randn([1, 3, 128, 1085]).to(device)\n",
    "print(type(x.to(device)))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "y = cnn(x)\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "z = invcnn(y)\n",
    "print(f\"z.shape = {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder and decoder\n",
    "\n",
    "Remark: the LSTM input size is constant, as it is derived from the dimension of the output of the cut resnet model, that is the product between the number of channels (256, fixed by ResNet34Cut) and the height of the output, that evaluates to `post_resnet_height = n_mels / 16 = 8` as long as `n_mels` remains 128.\n",
    "\n",
    "**Recall**\n",
    "In pytorch, a mono-directional LSTM set with `batch_first=True`, having `num_lstm_layers` LSTMs stacked on each other, takes as inputs:\n",
    "- the input sequence `x`, of shape `[batch, sequence, H_out]`\n",
    "- the initial hidden state `h_0`, of shape `[num_lstm_layers, batch, H_out]`\n",
    "- the initial cell state `c_0`, of shape  `[num_lstm_layers, batch, H_out]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cnn_height: int = int(N_MELS / 4) - 3\n",
    "lstm_input_size: int = num_channels[-1] * post_cnn_height\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 2048,\n",
    "        num_lstm_layers: int = 1,\n",
    "        output_size: int = 2048\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.cnn = CNNLayer()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.h0 = nn.parameter.Parameter(torch.zeros(hidden_size))\n",
    "        self.c0 = nn.parameter.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.init_state = None\n",
    "    \n",
    "    def set_init_state(self, s: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        self.init_state = s\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(L=16×N+1)]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.cnn(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        length = x.shape[-1]\n",
    "        x = x.view(batch_size, -1, length).swapaxes(1, 2)\n",
    "\n",
    "        # x.shape = [batch(B), length(N+1), height(lstm_input_size=2048)]\n",
    "        if self.init_state is not None:\n",
    "            output, (hn, cn) = self.lstm(x, self.init_state)\n",
    "            self.init_state = None\n",
    "        else:\n",
    "            h0 = self.h0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            c0 = self.c0.repeat(batch_size, 1).unsqueeze(0)\n",
    "            output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # hn.shape = [num_lstm_layers, batch(B), hidden]\n",
    "        x = self.fc(hn.squeeze(0))\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # x.shape = [batch(B), output_size]\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(output_size, hidden_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.const_x = nn.parameter.Parameter(torch.zeros(lstm_input_size))\n",
    "        print(f\"const_x shape: {self.const_x.shape}\")\n",
    "        self.h0 = nn.parameter.Parameter(torch.zeros(lstm_input_size))\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            proj_size=lstm_input_size,\n",
    "            batch_first=True,\n",
    "            # no need to specify sequence length\n",
    "        )\n",
    "        self.reverse_cnn = InverseCNNLayer()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def set_output_sequence_length(self, spec_length: int):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output spectrograms of length `length`, \n",
    "        parameterizing the LSTM input.\n",
    "        \"\"\"\n",
    "        length = int((spec_length-1) // 4 - 1)\n",
    "        self.sequence_length = length\n",
    "    \n",
    "    def set_output_sequence_length_seconds(self, seconds: float):\n",
    "        \"\"\"\n",
    "        Sets the decoder to output approximately `seconds` seconds spectrograms.\n",
    "        Relies on the MelSpectrogram class used, obviously.\n",
    "\n",
    "        Dumby interpolation from 801 spectrogram length corresponding to ±20s,\n",
    "        hence 40 length is 1s.\n",
    "        \"\"\"\n",
    "        self.set_output_sequence_length(int(40 * seconds))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.shape = [batch(B), output_size]\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # x.shape = [batch(B), hidden_size]\n",
    "        lstm_input = self.const_x.repeat(batch_size, self.sequence_length, 1).to(device)\n",
    "        h0 = self.h0.repeat(batch_size, 1).unsqueeze(0)\n",
    "        x, (hn, cn) = self.lstm(lstm_input, (h0, x.unsqueeze(0)))\n",
    "        \n",
    "        # x.shape = [batch(B), length(N+1), hidden_size]\n",
    "        x = x.swapaxes(1, 2).view(batch_size, num_channels[-1], post_cnn_height, self.sequence_length)\n",
    "\n",
    "        # x.shape = [batch(B), channels(256), height(8), length(N+1)]\n",
    "        x = self.reverse_cnn(x)\n",
    "\n",
    "        # x.shape = [batch(B), channels(3), n_mels(128), spec_length(16×N+1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lstm_input_size=2048,\n",
    "        hidden_size=2048,\n",
    "        sequence_length=51,\n",
    "        num_lstm_layers=1\n",
    "    ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            lstm_input_size=lstm_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            sequence_length=sequence_length,\n",
    "            num_lstm_layers=num_lstm_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const_x shape: torch.Size([580])\n",
      "v.shape = torch.Size([5, 2048])\n",
      "lstm input shapes: lstm input: torch.Size([5, 203, 580]), x: torch.Size([5, 2048]), h0: torch.Size([1, 5, 580])\n",
      "x.shape after lstm: torch.Size([5, 203, 580])\n",
      "y.shape = torch.Size([5, 3, 128, 817])\n"
     ]
    }
   ],
   "source": [
    "# testing...\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "x = torch.randn([5, 3, 128, 817])\n",
    "decoder.set_output_sequence_length(817)\n",
    "v = encoder(x.to(device))\n",
    "print(f\"v.shape = {v.shape}\")\n",
    "y = decoder(v)\n",
    "print(f\"y.shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training the model\n",
    "\n",
    "### 3.0 Utility for storing losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossesHandler(object):\n",
    "    def __init__(self, test_name: str, train_name: str):\n",
    "        self.ftest = open(test_name, \"w\")\n",
    "        self.ftrain = open(train_name, \"w\")\n",
    "        # self.losses[e][i] contains the loss at epoch e for elm i\n",
    "        self.train_losses = [[]]\n",
    "        self.test_losses = [[]]\n",
    "        self.spec_lengths = []\n",
    "        self.cur_epoch = 0\n",
    "    \n",
    "    def add_test_loss(self, test_loss: float):\n",
    "        self.test_losses[self.cur_epoch].append(test_loss)\n",
    "    \n",
    "    def add_train_loss(self, train_loss: float):\n",
    "        self.train_losses[self.cur_epoch].append(train_loss)\n",
    "    \n",
    "    def next_epoch(self, past_spec_length):\n",
    "        self.train_losses.append([])\n",
    "        self.test_losses.append([])\n",
    "        self.spec_lengths.append(past_spec_length)\n",
    "        self.cur_epoch += 1\n",
    "    \n",
    "    def write_epoch_losses(self):\n",
    "        self.ftrain.write(\", \".join([str(l) for l in self.train_losses[self.cur_epoch]]) + \"\\n\")\n",
    "        self.ftest.write(\", \".join([str(l) for l in self.test_losses[self.cur_epoch]]) + \"\\n\")\n",
    "    \n",
    "    def write_averages(self, filename: str):\n",
    "        f = open(filename, \"w\")\n",
    "        f.write(\"Epoch number,Epoch spec length,Average test loss,Average train loss\\n\")\n",
    "        for i, (test, train, spec_length) in enumerate(zip(self.test_losses, self.train_losses, self.spec_lengths)):\n",
    "            avg_test, avg_train = [sum(x) / len(x) for x in [test, train]]\n",
    "            f.write(f\"{i},{spec_length},{avg_test},{avg_train}\\n\")\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.ftrain.close()\n",
    "        self.ftest.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='22' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.33% [22/300 19:22<4:04:48]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1: avg test loss: 645098.5625, avg train loss: 955101.6875, Spec length: 897<p>Epoch 2: avg test loss: 1226202.125, avg train loss: 1070240.875, Spec length: 913<p>Epoch 3: avg test loss: 1156954.375, avg train loss: 1151413.0, Spec length: 993<p>Epoch 4: avg test loss: 1112220.125, avg train loss: 921992.1875, Spec length: 1041<p>Epoch 5: avg test loss: 840140.0, avg train loss: 789425.3125, Spec length: 961<p>Epoch 6: avg test loss: 1018719.5, avg train loss: 972616.375, Spec length: 817<p>Epoch 7: avg test loss: 1257447.25, avg train loss: 1182326.875, Spec length: 961<p>Epoch 8: avg test loss: 866557.0, avg train loss: 1154099.375, Spec length: 913<p>Epoch 9: avg test loss: 659866.75, avg train loss: 747202.5, Spec length: 961<p>Epoch 10: avg test loss: 1282420.375, avg train loss: 920913.0625, Spec length: 993<p>Epoch 11: avg test loss: 1004019.5, avg train loss: 1142498.25, Spec length: 977<p>Epoch 12: avg test loss: 596693.1875, avg train loss: 816215.0, Spec length: 817<p>Epoch 13: avg test loss: 1125727.25, avg train loss: 1001460.375, Spec length: 1041<p>Epoch 14: avg test loss: 957049.0, avg train loss: 1148411.0, Spec length: 913<p>Epoch 15: avg test loss: 1091797.875, avg train loss: 936288.625, Spec length: 961<p>Epoch 16: avg test loss: 1141175.75, avg train loss: 899397.4375, Spec length: 1057<p>Epoch 17: avg test loss: 1039933.4375, avg train loss: 846035.8125, Spec length: 929<p>Epoch 18: avg test loss: 1051071.0, avg train loss: 1098773.875, Spec length: 881<p>Epoch 19: avg test loss: 1095749.375, avg train loss: 1103707.75, Spec length: 929<p>Epoch 20: avg test loss: 1197121.75, avg train loss: 989191.75, Spec length: 1041<p>Epoch 21: avg test loss: 887618.875, avg train loss: 887306.5, Spec length: 929<p>Epoch 22: avg test loss: 926720.875, avg train loss: 885219.625, Spec length: 961<p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='21' class='' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.00% [21/140 00:06<00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#grad_norm = torch.norm(\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#    torch.stack([torch.norm(p.grad.detach(), 2) for p in decoder.parameters()]), 2)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Test:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "epoch_backup_spacing = 1\n",
    "h = LossesHandler(\"test_losses-scnn.csv\", \"train_losses-scnn.csv\")\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# spec lengths chosen randomly at each epochs\n",
    "num_choices = int((max_spec_length - min_spec_length) / 16)\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "master = master_bar(range(1, num_epochs+1))\n",
    "\n",
    "for epoch in master:\n",
    "    r = int(torch.randint(num_choices, size=(1, 1)))\n",
    "    l = min_spec_length + r * 16\n",
    "    train_loader.dataset.dataset.transform = make_spec_transform(l)\n",
    "    decoder.set_output_sequence_length(l)\n",
    "    # Train: \n",
    "    for x, _genre in (\n",
    "        progress_bar(train_loader, parent=master) if master is not None else train_loader\n",
    "    ):\n",
    "        x = x.to(device)\n",
    "        encoded = encoder(x)\n",
    "        pred = decoder(encoded)\n",
    "        loss = criterion(x, pred) \n",
    "        h.add_train_loss(loss.detach().mean())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #grad_norm = torch.norm(\n",
    "        #    torch.stack([torch.norm(p.grad.detach(), 2) for p in decoder.parameters()]), 2)\n",
    "\n",
    "        # print(f\"Grad norm: {grad_norm}\")\n",
    "\n",
    "    \n",
    "    # Test:\n",
    "    with torch.no_grad():\n",
    "        for x, _genre in (\n",
    "            progress_bar(test_loader, parent=master) if master is not None else test_loader\n",
    "        ):\n",
    "            x = x.to(device)\n",
    "            encoded = encoder(x)\n",
    "            pred = decoder(encoded)\n",
    "            loss = criterion(x, pred)\n",
    "            h.add_test_loss(loss.mean())\n",
    "    \n",
    "    h.write_epoch_losses()\n",
    "    h.next_epoch(l)\n",
    "\n",
    "    if epoch % epoch_backup_spacing == 0:\n",
    "        test_avg, train_avg = [sum(x) / len(x) for x in [\n",
    "            h.test_losses[h.cur_epoch-1], h.train_losses[h.cur_epoch-1]\n",
    "        ]]\n",
    "        master.write(f\"Epoch {epoch}: avg test loss: {test_avg}, avg train loss: {train_avg}, \" +\n",
    "                     f\"Spec length: {l}\")\n",
    "        torch.save(encoder.state_dict(), \"encoder_backup-scnn.pt\")\n",
    "        torch.save(decoder.state_dict(), \"decoder_backup-scnn.pt\")\n",
    "        h.write_averages(\"averages-backup.csv\")\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder-scnn.pt\")\n",
    "torch.save(decoder.state_dict(), \"decoder-scnn.pt\")\n",
    "h.write_averages(\"averages-scnn.csv\")\n",
    "print(\"Hey it's ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(params))\n",
    "#for i, p in enumerate(params):\n",
    "    #n = torch.norm(p.grad.detach(), 2)\n",
    "    #print(f\"{i} -> {n}\")\n",
    "for i, p in enumerate(params):\n",
    "    print(f\"{i} -> {torch.norm(p.grad.detach(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_encoder = \"model-encoder.pt\"\n",
    "# path_decoder = \"model-decoder.pt\"\n",
    "# torch.save(encoder.state_dict(), path_encoder)\n",
    "# torch.save(decoder.state_dict(), path_decoder)\n",
    "\n",
    "# reload: \n",
    "# encoder = Encoder()\n",
    "# decoder = Decoder()\n",
    "# encoder.load_state_dict(torch.load(path_encoder))\n",
    "# decoder.load_state_dict(torch.load(path_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avgs = pd.read_csv(\"averages.csv\")\n",
    "plt.figure()\n",
    "plt.plot(avgs[\"Epoch number\"], avgs[\"Average test loss\"])\n",
    "plt.plot(avgs[\"Epoch number\"], avgs[\"Average train loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(7)\n",
    "x = x.expand(4, 2, 1)\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "492dea7e935c1c914fc29e6b01e49fd41d40ab73afca215ec71ee5791b9738b1"
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
